{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3-Cheetah_Half_March-2022.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPpAwhMXkmH3Ltj2JS/2hdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prahladpunia/AI/blob/main/TD3_Cheetah_Half_March_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Twin-Delayed DDPG - Half_Cheetah"
      ],
      "metadata": {
        "id": "oxOhwnwZ9ICY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the packages"
      ],
      "metadata": {
        "id": "TANp7F8U9cV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "touPzUTP8Zni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ada5128-113c-41c4-cea2-783003e1694f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 90.8 MB 1.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pybullet #Motion detection collusion gaming"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "W9YVSdUN95Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "zITCEKS691mt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: We initialize the Experience Replay memory"
      ],
      "metadata": {
        "id": "j4V-vR3N-IWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "dJcRwosU-Jc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ],
      "metadata": {
        "id": "kiWO8LcPTHSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "YMjJm_DCTD6E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ],
      "metadata": {
        "id": "qNg1LbIrBl6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "metadata": {
        "id": "aGobE8tRBtni"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Steps 4 to 15: Training Process"
      ],
      "metadata": {
        "id": "fRuFVlyUJcIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "y7K5d5y4JghN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the whole Training Process into a class\n"
      ],
      "metadata": {
        "id": "pq6WHPoXKg5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "                 "
      ],
      "metadata": {
        "id": "FR-1vWNsKUJ5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a function to evaluate the policy by calculating its average reward over #10 episodes"
      ],
      "metadata": {
        "id": "LfgQATi-xeRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "metadata": {
        "id": "A8uaobDMwWgp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set the Parameters"
      ],
      "metadata": {
        "id": "IKL5HqJfx7k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # set to any continous environment\n",
        "seed = 0  # random seed number\n",
        "start_timesteps = 1e4 #no of iterations/timesteps before which the model randomly chooses an action, after which it starts to use policy\n",
        "eval_freq = 5e3 #How often the evaluation step is performed (after how many time steps)\n",
        "max_timesteps = 10e5 #Total no of iterations/timesteps\n",
        "save_models = True # Boolean checker wheter or not to save the pre-trained model\n",
        "expl_noise = 0.1 #Exploration Noise - STD value of exploration Gaussian Noise\n",
        "batch_size=100 # size of the batch\n",
        "discount = 0.99 #Discount factor gamma, used in the calculation of total discounted reward \n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 #STD of Gaussian Noise added to the Action for exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian Noise added to the actions (policy)\n",
        "policy_freq = 2 # No of iterations to wait before the policy network (Actor_model) is updated"
      ],
      "metadata": {
        "id": "pPNnvJ9wx_7B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a File Name for two saved models: the actor and critic models"
      ],
      "metadata": {
        "id": "iiLJdn1WXSd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLdb2jNXRkh",
        "outputId": "17484906-0a91-4080-b8c9-e2a986796510"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a folder inside which will save the Trained Models"
      ],
      "metadata": {
        "id": "rTWY5oBGXR17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "    os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "    os.makedirs(\"./pytorch_models\")"
      ],
      "metadata": {
        "id": "WDJPllC_wWz9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create pyBullet environment"
      ],
      "metadata": {
        "id": "Z1xuGeA4M1sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "u7adQB3jM68B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set seeds and get the necessary information on the STATES and ACTIONS in the chosen environment"
      ],
      "metadata": {
        "id": "LbrqEeIQNQb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n"
      ],
      "metadata": {
        "id": "-LOM49scNgYU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the POLICY NETWORK (the ACTOR MODEL)"
      ],
      "metadata": {
        "id": "GTL8HDEdODJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "metadata": {
        "id": "HTy15PvqODk6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Experience Replay Memory"
      ],
      "metadata": {
        "id": "CsgtaXgOPnEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "metadata": {
        "id": "hrVltw_KPng8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define a List where all the evaluation results over 10 episodes are stored"
      ],
      "metadata": {
        "id": "iG-HgNUVQM8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r203oCE4QaF0",
        "outputId": "c105089c-39ed-4f01-f04b-28ad9d1df0fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1429.426642\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a new folder directory in which the final results(videos of the agent) will be populated"
      ],
      "metadata": {
        "id": "EOZdTJK-Qxav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(base,name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path    \n",
        "work_dir = mkdir('exp','brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps    \n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "    env.reset()    "
      ],
      "metadata": {
        "id": "JiR98B_ZQxo6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the Variables"
      ],
      "metadata": {
        "id": "IclXoz-ZVD7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_timesteps=0\n",
        "timesteps_since_eval =0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "WbnNqnsnVEV-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "Xm6ojSVHV7A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olX0EqrKV7Yy",
        "outputId": "c8d44ca6-fbce-4260-9ce9-7dcc282457f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: -1346.6892649414808\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: -1270.4154682649082\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: -1196.2178757623308\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: -1080.7347250224545\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: -1265.7465744517924\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1399.110155\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: -1217.2016892609086\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: -1089.5937541848905\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: -1227.7711064007412\n",
            "Total Timesteps: 9000 Episode Num: 9 Reward: -1144.2065508602668\n",
            "Total Timesteps: 10000 Episode Num: 10 Reward: -1380.93543543816\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1586.154539\n",
            "---------------------------------------\n",
            "Total Timesteps: 11000 Episode Num: 11 Reward: -1538.561916598568\n",
            "Total Timesteps: 12000 Episode Num: 12 Reward: -1682.785590491847\n",
            "Total Timesteps: 13000 Episode Num: 13 Reward: -1156.116518231917\n",
            "Total Timesteps: 14000 Episode Num: 14 Reward: -622.9647324193495\n",
            "Total Timesteps: 15000 Episode Num: 15 Reward: 422.80813750695023\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1373.702987\n",
            "---------------------------------------\n",
            "Total Timesteps: 16000 Episode Num: 16 Reward: -1653.5522624290877\n",
            "Total Timesteps: 17000 Episode Num: 17 Reward: 143.877154176363\n",
            "Total Timesteps: 18000 Episode Num: 18 Reward: -543.4486069699162\n",
            "Total Timesteps: 19000 Episode Num: 19 Reward: -1523.9215077155468\n",
            "Total Timesteps: 20000 Episode Num: 20 Reward: -1615.2047787552956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1247.797424\n",
            "---------------------------------------\n",
            "Total Timesteps: 21000 Episode Num: 21 Reward: -1314.06002743142\n",
            "Total Timesteps: 22000 Episode Num: 22 Reward: -1270.636085508612\n",
            "Total Timesteps: 23000 Episode Num: 23 Reward: -803.2301389708601\n",
            "Total Timesteps: 24000 Episode Num: 24 Reward: 156.7325571967141\n",
            "Total Timesteps: 25000 Episode Num: 25 Reward: -1617.963148091506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1834.099323\n",
            "---------------------------------------\n",
            "Total Timesteps: 26000 Episode Num: 26 Reward: -2033.0425321172613\n",
            "Total Timesteps: 27000 Episode Num: 27 Reward: -1534.32365731225\n",
            "Total Timesteps: 28000 Episode Num: 28 Reward: -1366.575139218537\n",
            "Total Timesteps: 29000 Episode Num: 29 Reward: 501.2089816294372\n",
            "Total Timesteps: 30000 Episode Num: 30 Reward: -1344.9814347033248\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 151.568919\n",
            "---------------------------------------\n",
            "Total Timesteps: 31000 Episode Num: 31 Reward: 58.467610893262474\n",
            "Total Timesteps: 32000 Episode Num: 32 Reward: 460.857341865994\n",
            "Total Timesteps: 33000 Episode Num: 33 Reward: 237.1551436785569\n",
            "Total Timesteps: 34000 Episode Num: 34 Reward: -1453.7748592086302\n",
            "Total Timesteps: 35000 Episode Num: 35 Reward: -1351.2595757784013\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -765.324761\n",
            "---------------------------------------\n",
            "Total Timesteps: 36000 Episode Num: 36 Reward: -1035.7849106272542\n",
            "Total Timesteps: 37000 Episode Num: 37 Reward: 423.70743090214114\n",
            "Total Timesteps: 38000 Episode Num: 38 Reward: -792.5623383615208\n",
            "Total Timesteps: 39000 Episode Num: 39 Reward: 154.6030056884768\n",
            "Total Timesteps: 40000 Episode Num: 40 Reward: -1153.017699231618\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 292.039226\n",
            "---------------------------------------\n",
            "Total Timesteps: 41000 Episode Num: 41 Reward: 449.23971642742487\n",
            "Total Timesteps: 42000 Episode Num: 42 Reward: 56.10688359467768\n",
            "Total Timesteps: 43000 Episode Num: 43 Reward: -859.9032380390375\n",
            "Total Timesteps: 44000 Episode Num: 44 Reward: 386.4521410325331\n",
            "Total Timesteps: 45000 Episode Num: 45 Reward: 428.4502232874198\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 461.072639\n",
            "---------------------------------------\n",
            "Total Timesteps: 46000 Episode Num: 46 Reward: 346.54869385308115\n",
            "Total Timesteps: 47000 Episode Num: 47 Reward: 364.511509777108\n",
            "Total Timesteps: 48000 Episode Num: 48 Reward: 323.8612340667491\n",
            "Total Timesteps: 49000 Episode Num: 49 Reward: 229.55459588213893\n",
            "Total Timesteps: 50000 Episode Num: 50 Reward: 101.7031847835214\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 42.765067\n",
            "---------------------------------------\n",
            "Total Timesteps: 51000 Episode Num: 51 Reward: 470.4010644495721\n",
            "Total Timesteps: 52000 Episode Num: 52 Reward: -61.721384527962506\n",
            "Total Timesteps: 53000 Episode Num: 53 Reward: 196.25073000005065\n",
            "Total Timesteps: 54000 Episode Num: 54 Reward: 347.55532625075836\n",
            "Total Timesteps: 55000 Episode Num: 55 Reward: 472.7112572788259\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 435.269721\n",
            "---------------------------------------\n",
            "Total Timesteps: 56000 Episode Num: 56 Reward: 460.26347836480517\n",
            "Total Timesteps: 57000 Episode Num: 57 Reward: -55.89521873327089\n",
            "Total Timesteps: 58000 Episode Num: 58 Reward: 578.75302716466\n",
            "Total Timesteps: 59000 Episode Num: 59 Reward: -132.62760404212176\n",
            "Total Timesteps: 60000 Episode Num: 60 Reward: 421.5239951426814\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 292.938085\n",
            "---------------------------------------\n",
            "Total Timesteps: 61000 Episode Num: 61 Reward: 413.21863980912883\n",
            "Total Timesteps: 62000 Episode Num: 62 Reward: 506.3554197656252\n",
            "Total Timesteps: 63000 Episode Num: 63 Reward: 616.0696197061221\n",
            "Total Timesteps: 64000 Episode Num: 64 Reward: 644.1071485521736\n",
            "Total Timesteps: 65000 Episode Num: 65 Reward: 520.3021483200033\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 544.003627\n",
            "---------------------------------------\n",
            "Total Timesteps: 66000 Episode Num: 66 Reward: 251.59384771415804\n",
            "Total Timesteps: 67000 Episode Num: 67 Reward: 569.8817936538763\n",
            "Total Timesteps: 68000 Episode Num: 68 Reward: 612.7448517346028\n",
            "Total Timesteps: 69000 Episode Num: 69 Reward: 549.425415772967\n",
            "Total Timesteps: 70000 Episode Num: 70 Reward: 547.797825971403\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 573.662476\n",
            "---------------------------------------\n",
            "Total Timesteps: 71000 Episode Num: 71 Reward: 547.6737115317075\n",
            "Total Timesteps: 72000 Episode Num: 72 Reward: 547.2380700227818\n",
            "Total Timesteps: 73000 Episode Num: 73 Reward: 559.1021162725609\n",
            "Total Timesteps: 74000 Episode Num: 74 Reward: 542.6144825412695\n",
            "Total Timesteps: 75000 Episode Num: 75 Reward: 446.06693783789785\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 482.803963\n",
            "---------------------------------------\n",
            "Total Timesteps: 76000 Episode Num: 76 Reward: 484.52965499798506\n",
            "Total Timesteps: 77000 Episode Num: 77 Reward: 477.19007073698265\n",
            "Total Timesteps: 78000 Episode Num: 78 Reward: 662.8650622306252\n",
            "Total Timesteps: 79000 Episode Num: 79 Reward: 375.1719517710899\n",
            "Total Timesteps: 80000 Episode Num: 80 Reward: 754.0712294155273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 648.863216\n",
            "---------------------------------------\n",
            "Total Timesteps: 81000 Episode Num: 81 Reward: 654.768507862567\n",
            "Total Timesteps: 82000 Episode Num: 82 Reward: 664.7561787346222\n",
            "Total Timesteps: 83000 Episode Num: 83 Reward: 376.53230431852916\n",
            "Total Timesteps: 84000 Episode Num: 84 Reward: 550.6688515114413\n",
            "Total Timesteps: 85000 Episode Num: 85 Reward: -1682.733147490978\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1710.912848\n",
            "---------------------------------------\n",
            "Total Timesteps: 86000 Episode Num: 86 Reward: -1704.8883966712476\n",
            "Total Timesteps: 87000 Episode Num: 87 Reward: -1690.0330031168617\n",
            "Total Timesteps: 88000 Episode Num: 88 Reward: -1712.4137029796773\n",
            "Total Timesteps: 89000 Episode Num: 89 Reward: -1704.4283887369634\n",
            "Total Timesteps: 90000 Episode Num: 90 Reward: -1701.3178701128334\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1705.107153\n",
            "---------------------------------------\n",
            "Total Timesteps: 91000 Episode Num: 91 Reward: -1684.1171769489551\n",
            "Total Timesteps: 92000 Episode Num: 92 Reward: -1701.6664402849117\n",
            "Total Timesteps: 93000 Episode Num: 93 Reward: -1699.9884503057965\n",
            "Total Timesteps: 94000 Episode Num: 94 Reward: -1713.2724331374382\n",
            "Total Timesteps: 95000 Episode Num: 95 Reward: -1726.9869115909578\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1733.694648\n",
            "---------------------------------------\n",
            "Total Timesteps: 96000 Episode Num: 96 Reward: -1713.9862918922172\n",
            "Total Timesteps: 97000 Episode Num: 97 Reward: -1714.691851592974\n",
            "Total Timesteps: 98000 Episode Num: 98 Reward: -1733.8528975658112\n",
            "Total Timesteps: 99000 Episode Num: 99 Reward: 357.8791004706691\n",
            "Total Timesteps: 100000 Episode Num: 100 Reward: -1278.1634878838233\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 295.384056\n",
            "---------------------------------------\n",
            "Total Timesteps: 101000 Episode Num: 101 Reward: 568.9281919961063\n",
            "Total Timesteps: 102000 Episode Num: 102 Reward: 552.0890954108257\n",
            "Total Timesteps: 103000 Episode Num: 103 Reward: 590.3966739463616\n",
            "Total Timesteps: 104000 Episode Num: 104 Reward: 593.6495551796162\n",
            "Total Timesteps: 105000 Episode Num: 105 Reward: 572.311729118216\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 712.445015\n",
            "---------------------------------------\n",
            "Total Timesteps: 106000 Episode Num: 106 Reward: 760.5175921917789\n",
            "Total Timesteps: 107000 Episode Num: 107 Reward: 634.4144839485892\n",
            "Total Timesteps: 108000 Episode Num: 108 Reward: 643.4003703537551\n",
            "Total Timesteps: 109000 Episode Num: 109 Reward: 761.9688033283306\n",
            "Total Timesteps: 110000 Episode Num: 110 Reward: 581.2216074070872\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 791.266560\n",
            "---------------------------------------\n",
            "Total Timesteps: 111000 Episode Num: 111 Reward: 592.0696406285764\n",
            "Total Timesteps: 112000 Episode Num: 112 Reward: 766.9723298402645\n",
            "Total Timesteps: 113000 Episode Num: 113 Reward: 683.0048898002807\n",
            "Total Timesteps: 114000 Episode Num: 114 Reward: 782.1765198109593\n",
            "Total Timesteps: 115000 Episode Num: 115 Reward: -7.564834790547063\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 568.038241\n",
            "---------------------------------------\n",
            "Total Timesteps: 116000 Episode Num: 116 Reward: 576.051911506114\n",
            "Total Timesteps: 117000 Episode Num: 117 Reward: 764.9237914192363\n",
            "Total Timesteps: 118000 Episode Num: 118 Reward: 482.8195467412789\n",
            "Total Timesteps: 119000 Episode Num: 119 Reward: 671.6140846426607\n",
            "Total Timesteps: 120000 Episode Num: 120 Reward: 526.4652698622498\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 625.903008\n",
            "---------------------------------------\n",
            "Total Timesteps: 121000 Episode Num: 121 Reward: 603.1614784726279\n",
            "Total Timesteps: 122000 Episode Num: 122 Reward: 611.8054115996841\n",
            "Total Timesteps: 123000 Episode Num: 123 Reward: 646.0837557841446\n",
            "Total Timesteps: 124000 Episode Num: 124 Reward: 675.341921001448\n",
            "Total Timesteps: 125000 Episode Num: 125 Reward: 713.2867652352268\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 696.576522\n",
            "---------------------------------------\n",
            "Total Timesteps: 126000 Episode Num: 126 Reward: 775.0984059618434\n",
            "Total Timesteps: 127000 Episode Num: 127 Reward: 762.5946589652245\n",
            "Total Timesteps: 128000 Episode Num: 128 Reward: 774.4780131011989\n",
            "Total Timesteps: 129000 Episode Num: 129 Reward: 691.6279469819484\n",
            "Total Timesteps: 130000 Episode Num: 130 Reward: 639.9976772291652\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 792.237041\n",
            "---------------------------------------\n",
            "Total Timesteps: 131000 Episode Num: 131 Reward: 573.1582595910572\n",
            "Total Timesteps: 132000 Episode Num: 132 Reward: 739.1609531219474\n",
            "Total Timesteps: 133000 Episode Num: 133 Reward: 665.6405027168994\n",
            "Total Timesteps: 134000 Episode Num: 134 Reward: 683.8808129607573\n",
            "Total Timesteps: 135000 Episode Num: 135 Reward: 739.0097317504427\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 536.356553\n",
            "---------------------------------------\n",
            "Total Timesteps: 136000 Episode Num: 136 Reward: 489.07782073156415\n",
            "Total Timesteps: 137000 Episode Num: 137 Reward: 550.8637654937957\n",
            "Total Timesteps: 138000 Episode Num: 138 Reward: 639.2740047325727\n",
            "Total Timesteps: 139000 Episode Num: 139 Reward: 760.1801118388748\n",
            "Total Timesteps: 140000 Episode Num: 140 Reward: 791.0551975975255\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 709.655407\n",
            "---------------------------------------\n",
            "Total Timesteps: 141000 Episode Num: 141 Reward: 585.5338245624864\n",
            "Total Timesteps: 142000 Episode Num: 142 Reward: 662.4174000411524\n",
            "Total Timesteps: 143000 Episode Num: 143 Reward: 747.7090342176008\n",
            "Total Timesteps: 144000 Episode Num: 144 Reward: 700.9418172441547\n",
            "Total Timesteps: 145000 Episode Num: 145 Reward: 769.2445520188683\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 633.875414\n",
            "---------------------------------------\n",
            "Total Timesteps: 146000 Episode Num: 146 Reward: 663.8434502968914\n",
            "Total Timesteps: 147000 Episode Num: 147 Reward: 719.8612354023588\n",
            "Total Timesteps: 148000 Episode Num: 148 Reward: 762.8140063497472\n",
            "Total Timesteps: 149000 Episode Num: 149 Reward: 746.0707203928471\n",
            "Total Timesteps: 150000 Episode Num: 150 Reward: 713.4947424284534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.702150\n",
            "---------------------------------------\n",
            "Total Timesteps: 151000 Episode Num: 151 Reward: 677.8301760555944\n",
            "Total Timesteps: 152000 Episode Num: 152 Reward: 707.7526605348997\n",
            "Total Timesteps: 153000 Episode Num: 153 Reward: 592.6243239435445\n",
            "Total Timesteps: 154000 Episode Num: 154 Reward: 640.6987026378065\n",
            "Total Timesteps: 155000 Episode Num: 155 Reward: 735.5540788494084\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 737.148090\n",
            "---------------------------------------\n",
            "Total Timesteps: 156000 Episode Num: 156 Reward: 703.8582568592516\n",
            "Total Timesteps: 157000 Episode Num: 157 Reward: 667.7005754501383\n",
            "Total Timesteps: 158000 Episode Num: 158 Reward: 585.6896894914519\n",
            "Total Timesteps: 159000 Episode Num: 159 Reward: 721.698959325362\n",
            "Total Timesteps: 160000 Episode Num: 160 Reward: 623.2723237707277\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 633.649654\n",
            "---------------------------------------\n",
            "Total Timesteps: 161000 Episode Num: 161 Reward: 641.9353393392898\n",
            "Total Timesteps: 162000 Episode Num: 162 Reward: 727.0589788629478\n",
            "Total Timesteps: 163000 Episode Num: 163 Reward: 838.2155353079438\n",
            "Total Timesteps: 164000 Episode Num: 164 Reward: 561.6805561124846\n",
            "Total Timesteps: 165000 Episode Num: 165 Reward: 776.9563939186824\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 811.911846\n",
            "---------------------------------------\n",
            "Total Timesteps: 166000 Episode Num: 166 Reward: 839.3164998754163\n",
            "Total Timesteps: 167000 Episode Num: 167 Reward: 831.4873937098273\n",
            "Total Timesteps: 168000 Episode Num: 168 Reward: 801.6405118695667\n",
            "Total Timesteps: 169000 Episode Num: 169 Reward: 646.4463021796654\n",
            "Total Timesteps: 170000 Episode Num: 170 Reward: 628.1419666094909\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 672.804861\n",
            "---------------------------------------\n",
            "Total Timesteps: 171000 Episode Num: 171 Reward: 526.9173121388178\n",
            "Total Timesteps: 172000 Episode Num: 172 Reward: 455.0197372743787\n",
            "Total Timesteps: 173000 Episode Num: 173 Reward: 618.4443520277924\n",
            "Total Timesteps: 174000 Episode Num: 174 Reward: 741.3250007943128\n",
            "Total Timesteps: 175000 Episode Num: 175 Reward: 776.138443666109\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 623.441387\n",
            "---------------------------------------\n",
            "Total Timesteps: 176000 Episode Num: 176 Reward: 565.7712889601462\n",
            "Total Timesteps: 177000 Episode Num: 177 Reward: 687.6697520531759\n",
            "Total Timesteps: 178000 Episode Num: 178 Reward: 582.744041776435\n",
            "Total Timesteps: 179000 Episode Num: 179 Reward: 631.0812982361911\n",
            "Total Timesteps: 180000 Episode Num: 180 Reward: 837.6679266640186\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 790.986222\n",
            "---------------------------------------\n",
            "Total Timesteps: 181000 Episode Num: 181 Reward: 807.6949298461998\n",
            "Total Timesteps: 182000 Episode Num: 182 Reward: 821.7443690531885\n",
            "Total Timesteps: 183000 Episode Num: 183 Reward: 792.4842722102303\n",
            "Total Timesteps: 184000 Episode Num: 184 Reward: 792.0958566309881\n",
            "Total Timesteps: 185000 Episode Num: 185 Reward: 823.4430595788772\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 823.572263\n",
            "---------------------------------------\n",
            "Total Timesteps: 186000 Episode Num: 186 Reward: 807.9495053306151\n",
            "Total Timesteps: 187000 Episode Num: 187 Reward: 817.1720373304299\n",
            "Total Timesteps: 188000 Episode Num: 188 Reward: 826.0063741679883\n",
            "Total Timesteps: 189000 Episode Num: 189 Reward: 699.1796478665432\n",
            "Total Timesteps: 190000 Episode Num: 190 Reward: 818.7673237988139\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 831.511785\n",
            "---------------------------------------\n",
            "Total Timesteps: 191000 Episode Num: 191 Reward: 850.9965255321782\n",
            "Total Timesteps: 192000 Episode Num: 192 Reward: 717.8700229419655\n",
            "Total Timesteps: 193000 Episode Num: 193 Reward: 744.0642022325646\n",
            "Total Timesteps: 194000 Episode Num: 194 Reward: 761.6851510879278\n",
            "Total Timesteps: 195000 Episode Num: 195 Reward: 735.4614462760418\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 791.480594\n",
            "---------------------------------------\n",
            "Total Timesteps: 196000 Episode Num: 196 Reward: 773.2967885188453\n",
            "Total Timesteps: 197000 Episode Num: 197 Reward: 768.7181661409481\n",
            "Total Timesteps: 198000 Episode Num: 198 Reward: 790.076309225284\n",
            "Total Timesteps: 199000 Episode Num: 199 Reward: 773.6646999483105\n",
            "Total Timesteps: 200000 Episode Num: 200 Reward: 598.3144740472973\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 664.996921\n",
            "---------------------------------------\n",
            "Total Timesteps: 201000 Episode Num: 201 Reward: 590.9273533248605\n",
            "Total Timesteps: 202000 Episode Num: 202 Reward: 783.3171339051115\n",
            "Total Timesteps: 203000 Episode Num: 203 Reward: 746.5161205273765\n",
            "Total Timesteps: 204000 Episode Num: 204 Reward: 836.8924014908855\n",
            "Total Timesteps: 205000 Episode Num: 205 Reward: 716.8290507558506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 725.935307\n",
            "---------------------------------------\n",
            "Total Timesteps: 206000 Episode Num: 206 Reward: 692.4186170901756\n",
            "Total Timesteps: 207000 Episode Num: 207 Reward: 711.8249015235191\n",
            "Total Timesteps: 208000 Episode Num: 208 Reward: 855.7066782712326\n",
            "Total Timesteps: 209000 Episode Num: 209 Reward: 641.2867553460787\n",
            "Total Timesteps: 210000 Episode Num: 210 Reward: 648.198674421656\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 629.866722\n",
            "---------------------------------------\n",
            "Total Timesteps: 211000 Episode Num: 211 Reward: 609.2374969123002\n",
            "Total Timesteps: 212000 Episode Num: 212 Reward: 728.4387362947181\n",
            "Total Timesteps: 213000 Episode Num: 213 Reward: 670.5852598700092\n",
            "Total Timesteps: 214000 Episode Num: 214 Reward: 805.4030735420179\n",
            "Total Timesteps: 215000 Episode Num: 215 Reward: 728.8168087550616\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 755.683126\n",
            "---------------------------------------\n",
            "Total Timesteps: 216000 Episode Num: 216 Reward: 773.8750655999891\n",
            "Total Timesteps: 217000 Episode Num: 217 Reward: 786.720741818598\n",
            "Total Timesteps: 218000 Episode Num: 218 Reward: 736.635222773269\n",
            "Total Timesteps: 219000 Episode Num: 219 Reward: 841.1640015658727\n",
            "Total Timesteps: 220000 Episode Num: 220 Reward: 939.0635125307115\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 922.274703\n",
            "---------------------------------------\n",
            "Total Timesteps: 221000 Episode Num: 221 Reward: 929.0414132938473\n",
            "Total Timesteps: 222000 Episode Num: 222 Reward: 828.1410029922482\n",
            "Total Timesteps: 223000 Episode Num: 223 Reward: 793.2877832538647\n",
            "Total Timesteps: 224000 Episode Num: 224 Reward: 871.4996164129549\n",
            "Total Timesteps: 225000 Episode Num: 225 Reward: 889.2879476632015\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 892.743264\n",
            "---------------------------------------\n",
            "Total Timesteps: 226000 Episode Num: 226 Reward: 876.3757648265077\n",
            "Total Timesteps: 227000 Episode Num: 227 Reward: 864.8817142108211\n",
            "Total Timesteps: 228000 Episode Num: 228 Reward: 840.5170705886206\n",
            "Total Timesteps: 229000 Episode Num: 229 Reward: 847.1847086878877\n",
            "Total Timesteps: 230000 Episode Num: 230 Reward: 886.2656988995827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 696.236162\n",
            "---------------------------------------\n",
            "Total Timesteps: 231000 Episode Num: 231 Reward: 635.6095417738987\n",
            "Total Timesteps: 232000 Episode Num: 232 Reward: 855.3272545326472\n",
            "Total Timesteps: 233000 Episode Num: 233 Reward: 841.4800633534662\n",
            "Total Timesteps: 234000 Episode Num: 234 Reward: 881.4852719090877\n",
            "Total Timesteps: 235000 Episode Num: 235 Reward: 775.2212111672766\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 673.631247\n",
            "---------------------------------------\n",
            "Total Timesteps: 236000 Episode Num: 236 Reward: 752.6838088909444\n",
            "Total Timesteps: 237000 Episode Num: 237 Reward: 724.0314131863021\n",
            "Total Timesteps: 238000 Episode Num: 238 Reward: 916.727932882598\n",
            "Total Timesteps: 239000 Episode Num: 239 Reward: 924.7467531625796\n",
            "Total Timesteps: 240000 Episode Num: 240 Reward: 978.2175959647008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 810.393556\n",
            "---------------------------------------\n",
            "Total Timesteps: 241000 Episode Num: 241 Reward: 800.3663495560571\n",
            "Total Timesteps: 242000 Episode Num: 242 Reward: 995.4996219372363\n",
            "Total Timesteps: 243000 Episode Num: 243 Reward: 896.629381925258\n",
            "Total Timesteps: 244000 Episode Num: 244 Reward: 994.0976023378612\n",
            "Total Timesteps: 245000 Episode Num: 245 Reward: 969.7173643622601\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 964.547316\n",
            "---------------------------------------\n",
            "Total Timesteps: 246000 Episode Num: 246 Reward: 924.5693516048342\n",
            "Total Timesteps: 247000 Episode Num: 247 Reward: 985.8506945181547\n",
            "Total Timesteps: 248000 Episode Num: 248 Reward: 1002.9289954128303\n",
            "Total Timesteps: 249000 Episode Num: 249 Reward: 945.822316558033\n",
            "Total Timesteps: 250000 Episode Num: 250 Reward: 1042.1169631100447\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1087.234630\n",
            "---------------------------------------\n",
            "Total Timesteps: 251000 Episode Num: 251 Reward: 1112.3316677288803\n",
            "Total Timesteps: 252000 Episode Num: 252 Reward: 996.6550332490915\n",
            "Total Timesteps: 253000 Episode Num: 253 Reward: 990.4558125711995\n",
            "Total Timesteps: 254000 Episode Num: 254 Reward: 999.2649377544215\n",
            "Total Timesteps: 255000 Episode Num: 255 Reward: 1136.0624992784024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 861.302402\n",
            "---------------------------------------\n",
            "Total Timesteps: 256000 Episode Num: 256 Reward: 842.9658626856091\n",
            "Total Timesteps: 257000 Episode Num: 257 Reward: 858.1946189743921\n",
            "Total Timesteps: 258000 Episode Num: 258 Reward: 1059.488105156282\n",
            "Total Timesteps: 259000 Episode Num: 259 Reward: 923.643028960203\n",
            "Total Timesteps: 260000 Episode Num: 260 Reward: 1035.9245249996554\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1039.366681\n",
            "---------------------------------------\n",
            "Total Timesteps: 261000 Episode Num: 261 Reward: 932.01737240126\n",
            "Total Timesteps: 262000 Episode Num: 262 Reward: 1231.0163361245832\n",
            "Total Timesteps: 263000 Episode Num: 263 Reward: 1161.2248187614973\n",
            "Total Timesteps: 264000 Episode Num: 264 Reward: 1158.4395626896523\n",
            "Total Timesteps: 265000 Episode Num: 265 Reward: 1189.9579416488557\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1048.850650\n",
            "---------------------------------------\n",
            "Total Timesteps: 266000 Episode Num: 266 Reward: 1020.1618183401532\n",
            "Total Timesteps: 267000 Episode Num: 267 Reward: 1131.48050245955\n",
            "Total Timesteps: 268000 Episode Num: 268 Reward: 1238.4099269818062\n",
            "Total Timesteps: 269000 Episode Num: 269 Reward: 1114.4750496945937\n",
            "Total Timesteps: 270000 Episode Num: 270 Reward: 1273.178200832343\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1317.403972\n",
            "---------------------------------------\n",
            "Total Timesteps: 271000 Episode Num: 271 Reward: 1245.612240343299\n",
            "Total Timesteps: 272000 Episode Num: 272 Reward: 1140.0574500130965\n",
            "Total Timesteps: 273000 Episode Num: 273 Reward: 1154.7669609835104\n",
            "Total Timesteps: 274000 Episode Num: 274 Reward: 1312.6048367521435\n",
            "Total Timesteps: 275000 Episode Num: 275 Reward: 1138.9181821013385\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1160.684491\n",
            "---------------------------------------\n",
            "Total Timesteps: 276000 Episode Num: 276 Reward: 1133.4260876289509\n",
            "Total Timesteps: 277000 Episode Num: 277 Reward: 1252.9619654997578\n",
            "Total Timesteps: 278000 Episode Num: 278 Reward: 1192.3018265877627\n",
            "Total Timesteps: 279000 Episode Num: 279 Reward: 1260.4187095604284\n",
            "Total Timesteps: 280000 Episode Num: 280 Reward: 1113.7788061261263\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1217.947352\n",
            "---------------------------------------\n",
            "Total Timesteps: 281000 Episode Num: 281 Reward: 1180.2567418343403\n",
            "Total Timesteps: 282000 Episode Num: 282 Reward: 970.3272883323796\n",
            "Total Timesteps: 283000 Episode Num: 283 Reward: 1280.5525556476086\n",
            "Total Timesteps: 284000 Episode Num: 284 Reward: 1272.2655313968417\n",
            "Total Timesteps: 285000 Episode Num: 285 Reward: 1217.542676565311\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1439.222865\n",
            "---------------------------------------\n",
            "Total Timesteps: 286000 Episode Num: 286 Reward: 1418.214115176135\n",
            "Total Timesteps: 287000 Episode Num: 287 Reward: 1284.4104903453926\n",
            "Total Timesteps: 288000 Episode Num: 288 Reward: 1379.6799873447599\n",
            "Total Timesteps: 289000 Episode Num: 289 Reward: 1428.0108381596751\n",
            "Total Timesteps: 290000 Episode Num: 290 Reward: 1379.821994073801\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1443.105200\n",
            "---------------------------------------\n",
            "Total Timesteps: 291000 Episode Num: 291 Reward: 1401.794511520497\n",
            "Total Timesteps: 292000 Episode Num: 292 Reward: 1349.9308662184446\n",
            "Total Timesteps: 293000 Episode Num: 293 Reward: 1420.6217776136477\n",
            "Total Timesteps: 294000 Episode Num: 294 Reward: 1338.5217662920556\n",
            "Total Timesteps: 295000 Episode Num: 295 Reward: 1323.4465920397731\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1431.349182\n",
            "---------------------------------------\n",
            "Total Timesteps: 296000 Episode Num: 296 Reward: 1386.480908247332\n",
            "Total Timesteps: 297000 Episode Num: 297 Reward: 1487.345777275434\n",
            "Total Timesteps: 298000 Episode Num: 298 Reward: 1437.028938450177\n",
            "Total Timesteps: 299000 Episode Num: 299 Reward: 1515.056860200769\n",
            "Total Timesteps: 300000 Episode Num: 300 Reward: 1358.3452026566056\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1464.467031\n",
            "---------------------------------------\n",
            "Total Timesteps: 301000 Episode Num: 301 Reward: 1409.9488330763736\n",
            "Total Timesteps: 302000 Episode Num: 302 Reward: 1478.4951784522611\n",
            "Total Timesteps: 303000 Episode Num: 303 Reward: 1447.1015422845219\n",
            "Total Timesteps: 304000 Episode Num: 304 Reward: 1474.002293520188\n",
            "Total Timesteps: 305000 Episode Num: 305 Reward: 1501.8497362952687\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1527.390725\n",
            "---------------------------------------\n",
            "Total Timesteps: 306000 Episode Num: 306 Reward: 1489.5494880230076\n",
            "Total Timesteps: 307000 Episode Num: 307 Reward: 1496.0250486086616\n",
            "Total Timesteps: 308000 Episode Num: 308 Reward: 1451.1250288584547\n",
            "Total Timesteps: 309000 Episode Num: 309 Reward: 1528.6412739101452\n",
            "Total Timesteps: 310000 Episode Num: 310 Reward: 1525.932618625372\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1472.716193\n",
            "---------------------------------------\n",
            "Total Timesteps: 311000 Episode Num: 311 Reward: 1443.0096734316032\n",
            "Total Timesteps: 312000 Episode Num: 312 Reward: 1444.8497160891\n",
            "Total Timesteps: 313000 Episode Num: 313 Reward: 1511.8992789170165\n",
            "Total Timesteps: 314000 Episode Num: 314 Reward: 1563.8336691048523\n",
            "Total Timesteps: 315000 Episode Num: 315 Reward: 1514.8888053074568\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1539.538624\n",
            "---------------------------------------\n",
            "Total Timesteps: 316000 Episode Num: 316 Reward: 1516.3462991331114\n",
            "Total Timesteps: 317000 Episode Num: 317 Reward: 1522.9070150964344\n",
            "Total Timesteps: 318000 Episode Num: 318 Reward: 1497.9689662412543\n",
            "Total Timesteps: 319000 Episode Num: 319 Reward: 1551.336372207106\n",
            "Total Timesteps: 320000 Episode Num: 320 Reward: 1510.6021091633936\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1508.270247\n",
            "---------------------------------------\n",
            "Total Timesteps: 321000 Episode Num: 321 Reward: 1499.2552387404262\n",
            "Total Timesteps: 322000 Episode Num: 322 Reward: 1461.6018379213708\n",
            "Total Timesteps: 323000 Episode Num: 323 Reward: 1550.0914994715106\n",
            "Total Timesteps: 324000 Episode Num: 324 Reward: 1565.305764556488\n",
            "Total Timesteps: 325000 Episode Num: 325 Reward: 1564.2274089159043\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1592.365540\n",
            "---------------------------------------\n",
            "Total Timesteps: 326000 Episode Num: 326 Reward: 1572.686134140427\n",
            "Total Timesteps: 327000 Episode Num: 327 Reward: 1537.7809266843876\n",
            "Total Timesteps: 328000 Episode Num: 328 Reward: 1425.1700374639986\n",
            "Total Timesteps: 329000 Episode Num: 329 Reward: 1552.7580018429517\n",
            "Total Timesteps: 330000 Episode Num: 330 Reward: 1516.5384099089963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1597.543953\n",
            "---------------------------------------\n",
            "Total Timesteps: 331000 Episode Num: 331 Reward: 1562.8880755647936\n",
            "Total Timesteps: 332000 Episode Num: 332 Reward: 1600.046455588547\n",
            "Total Timesteps: 333000 Episode Num: 333 Reward: 1557.9989631142532\n",
            "Total Timesteps: 334000 Episode Num: 334 Reward: 1468.718976760512\n",
            "Total Timesteps: 335000 Episode Num: 335 Reward: 1562.4391264216626\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1632.687356\n",
            "---------------------------------------\n",
            "Total Timesteps: 336000 Episode Num: 336 Reward: 1598.9903752322082\n",
            "Total Timesteps: 337000 Episode Num: 337 Reward: 1550.1069599771797\n",
            "Total Timesteps: 338000 Episode Num: 338 Reward: 1421.8896708133836\n",
            "Total Timesteps: 339000 Episode Num: 339 Reward: 1491.6903054675863\n",
            "Total Timesteps: 340000 Episode Num: 340 Reward: 1549.850237328945\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1567.234093\n",
            "---------------------------------------\n",
            "Total Timesteps: 341000 Episode Num: 341 Reward: 1553.9155310543076\n",
            "Total Timesteps: 342000 Episode Num: 342 Reward: 1497.5569206522878\n",
            "Total Timesteps: 343000 Episode Num: 343 Reward: 1588.559194666413\n",
            "Total Timesteps: 344000 Episode Num: 344 Reward: 1547.6826671151716\n",
            "Total Timesteps: 345000 Episode Num: 345 Reward: 1537.3752752018513\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1523.510416\n",
            "---------------------------------------\n",
            "Total Timesteps: 346000 Episode Num: 346 Reward: 1504.8110364605081\n",
            "Total Timesteps: 347000 Episode Num: 347 Reward: 1505.5697794680182\n",
            "Total Timesteps: 348000 Episode Num: 348 Reward: 1572.2972297833364\n",
            "Total Timesteps: 349000 Episode Num: 349 Reward: 1556.9484816848587\n",
            "Total Timesteps: 350000 Episode Num: 350 Reward: 1515.142605872224\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1660.694028\n",
            "---------------------------------------\n",
            "Total Timesteps: 351000 Episode Num: 351 Reward: 1621.0949172699916\n",
            "Total Timesteps: 352000 Episode Num: 352 Reward: 1492.7142492053981\n",
            "Total Timesteps: 353000 Episode Num: 353 Reward: 1595.882430408227\n",
            "Total Timesteps: 354000 Episode Num: 354 Reward: 1552.5245777753557\n",
            "Total Timesteps: 355000 Episode Num: 355 Reward: 1586.343437734788\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1587.887005\n",
            "---------------------------------------\n",
            "Total Timesteps: 356000 Episode Num: 356 Reward: 1572.6234760859838\n",
            "Total Timesteps: 357000 Episode Num: 357 Reward: 1602.9894025543433\n",
            "Total Timesteps: 358000 Episode Num: 358 Reward: 1580.3323119023366\n",
            "Total Timesteps: 359000 Episode Num: 359 Reward: 1538.6088169044292\n",
            "Total Timesteps: 360000 Episode Num: 360 Reward: 1489.1162462115503\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1516.902707\n",
            "---------------------------------------\n",
            "Total Timesteps: 361000 Episode Num: 361 Reward: 1517.4614654896393\n",
            "Total Timesteps: 362000 Episode Num: 362 Reward: 1536.7484466564724\n",
            "Total Timesteps: 363000 Episode Num: 363 Reward: 1613.8291607838155\n",
            "Total Timesteps: 364000 Episode Num: 364 Reward: 1248.268482926229\n",
            "Total Timesteps: 365000 Episode Num: 365 Reward: 1587.9781518377422\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1618.915153\n",
            "---------------------------------------\n",
            "Total Timesteps: 366000 Episode Num: 366 Reward: 1599.021174541583\n",
            "Total Timesteps: 367000 Episode Num: 367 Reward: 1532.169139596198\n",
            "Total Timesteps: 368000 Episode Num: 368 Reward: 1631.8855435131823\n",
            "Total Timesteps: 369000 Episode Num: 369 Reward: 1592.2147307174805\n",
            "Total Timesteps: 370000 Episode Num: 370 Reward: 1578.9180323405706\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1610.868110\n",
            "---------------------------------------\n",
            "Total Timesteps: 371000 Episode Num: 371 Reward: 1599.6027861463117\n",
            "Total Timesteps: 372000 Episode Num: 372 Reward: 1643.5557477416273\n",
            "Total Timesteps: 373000 Episode Num: 373 Reward: 1580.568571472647\n",
            "Total Timesteps: 374000 Episode Num: 374 Reward: 1581.7575531690045\n",
            "Total Timesteps: 375000 Episode Num: 375 Reward: 1606.398284761631\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1631.363835\n",
            "---------------------------------------\n",
            "Total Timesteps: 376000 Episode Num: 376 Reward: 1603.9500531757828\n",
            "Total Timesteps: 377000 Episode Num: 377 Reward: 1648.7300186559953\n",
            "Total Timesteps: 378000 Episode Num: 378 Reward: 1596.3174345942746\n",
            "Total Timesteps: 379000 Episode Num: 379 Reward: 1558.704991982124\n",
            "Total Timesteps: 380000 Episode Num: 380 Reward: 1571.5862088222814\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1617.222533\n",
            "---------------------------------------\n",
            "Total Timesteps: 381000 Episode Num: 381 Reward: 1596.222841558911\n",
            "Total Timesteps: 382000 Episode Num: 382 Reward: 1478.8454325265225\n",
            "Total Timesteps: 383000 Episode Num: 383 Reward: 1581.7851475098078\n",
            "Total Timesteps: 384000 Episode Num: 384 Reward: 1621.5088497912045\n",
            "Total Timesteps: 385000 Episode Num: 385 Reward: 1626.229682569051\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1582.336401\n",
            "---------------------------------------\n",
            "Total Timesteps: 386000 Episode Num: 386 Reward: 1547.452481669035\n",
            "Total Timesteps: 387000 Episode Num: 387 Reward: 1590.2530369822873\n",
            "Total Timesteps: 388000 Episode Num: 388 Reward: 1624.96867884389\n",
            "Total Timesteps: 389000 Episode Num: 389 Reward: 1659.840738061767\n",
            "Total Timesteps: 390000 Episode Num: 390 Reward: 1670.2655589028113\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1751.096905\n",
            "---------------------------------------\n",
            "Total Timesteps: 391000 Episode Num: 391 Reward: 1737.5938094974656\n",
            "Total Timesteps: 392000 Episode Num: 392 Reward: 1700.9739905344418\n",
            "Total Timesteps: 393000 Episode Num: 393 Reward: 1729.2566823225418\n",
            "Total Timesteps: 394000 Episode Num: 394 Reward: 1679.774913052704\n",
            "Total Timesteps: 395000 Episode Num: 395 Reward: 1669.4844538567409\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1606.167072\n",
            "---------------------------------------\n",
            "Total Timesteps: 396000 Episode Num: 396 Reward: 1592.5826522601092\n",
            "Total Timesteps: 397000 Episode Num: 397 Reward: 1620.5732892171281\n",
            "Total Timesteps: 398000 Episode Num: 398 Reward: 1666.901362205617\n",
            "Total Timesteps: 399000 Episode Num: 399 Reward: 1791.3047535670678\n",
            "Total Timesteps: 400000 Episode Num: 400 Reward: 1668.1552083283473\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1834.564200\n",
            "---------------------------------------\n",
            "Total Timesteps: 401000 Episode Num: 401 Reward: 1816.6890240777736\n",
            "Total Timesteps: 402000 Episode Num: 402 Reward: 1685.075423405786\n",
            "Total Timesteps: 403000 Episode Num: 403 Reward: 1590.6855620044425\n",
            "Total Timesteps: 404000 Episode Num: 404 Reward: 1675.5559605116032\n",
            "Total Timesteps: 405000 Episode Num: 405 Reward: 1713.6376152348798\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1749.631180\n",
            "---------------------------------------\n",
            "Total Timesteps: 406000 Episode Num: 406 Reward: 1715.4302835335786\n",
            "Total Timesteps: 407000 Episode Num: 407 Reward: 1649.2006586622415\n",
            "Total Timesteps: 408000 Episode Num: 408 Reward: 1616.5638574503837\n",
            "Total Timesteps: 409000 Episode Num: 409 Reward: 1784.0241013022967\n",
            "Total Timesteps: 410000 Episode Num: 410 Reward: 1697.3376316734627\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1824.412545\n",
            "---------------------------------------\n",
            "Total Timesteps: 411000 Episode Num: 411 Reward: 1802.854583004609\n",
            "Total Timesteps: 412000 Episode Num: 412 Reward: 1763.6431683409367\n",
            "Total Timesteps: 413000 Episode Num: 413 Reward: 1855.718881074501\n",
            "Total Timesteps: 414000 Episode Num: 414 Reward: 1807.5495532439086\n",
            "Total Timesteps: 415000 Episode Num: 415 Reward: 1758.8900670377607\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1827.732687\n",
            "---------------------------------------\n",
            "Total Timesteps: 416000 Episode Num: 416 Reward: 1788.1608552761581\n",
            "Total Timesteps: 417000 Episode Num: 417 Reward: 1560.4743742656717\n",
            "Total Timesteps: 418000 Episode Num: 418 Reward: 1774.6959028310011\n",
            "Total Timesteps: 419000 Episode Num: 419 Reward: 1649.6740703821058\n",
            "Total Timesteps: 420000 Episode Num: 420 Reward: 1729.426317540628\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1738.519734\n",
            "---------------------------------------\n",
            "Total Timesteps: 421000 Episode Num: 421 Reward: 1737.8321445050021\n",
            "Total Timesteps: 422000 Episode Num: 422 Reward: 1666.1828862643065\n",
            "Total Timesteps: 423000 Episode Num: 423 Reward: 1646.0871894692293\n",
            "Total Timesteps: 424000 Episode Num: 424 Reward: 1643.625332259284\n",
            "Total Timesteps: 425000 Episode Num: 425 Reward: 1821.2909954356485\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1741.290752\n",
            "---------------------------------------\n",
            "Total Timesteps: 426000 Episode Num: 426 Reward: 1768.5422241853885\n",
            "Total Timesteps: 427000 Episode Num: 427 Reward: 1768.4514612996063\n",
            "Total Timesteps: 428000 Episode Num: 428 Reward: 1725.5835066117577\n",
            "Total Timesteps: 429000 Episode Num: 429 Reward: 1703.2519017410639\n",
            "Total Timesteps: 430000 Episode Num: 430 Reward: 1744.1403225020026\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1795.655950\n",
            "---------------------------------------\n",
            "Total Timesteps: 431000 Episode Num: 431 Reward: 1777.975383438541\n",
            "Total Timesteps: 432000 Episode Num: 432 Reward: 1770.161159035841\n",
            "Total Timesteps: 433000 Episode Num: 433 Reward: 1668.9106685872096\n",
            "Total Timesteps: 434000 Episode Num: 434 Reward: 1822.689709746714\n",
            "Total Timesteps: 435000 Episode Num: 435 Reward: 1646.8197778008007\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1823.683660\n",
            "---------------------------------------\n",
            "Total Timesteps: 436000 Episode Num: 436 Reward: 1803.3476774223743\n",
            "Total Timesteps: 437000 Episode Num: 437 Reward: 1818.9816752448082\n",
            "Total Timesteps: 438000 Episode Num: 438 Reward: 1732.394969667183\n",
            "Total Timesteps: 439000 Episode Num: 439 Reward: 1721.1169900799785\n",
            "Total Timesteps: 440000 Episode Num: 440 Reward: 1754.8849465872374\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1882.705701\n",
            "---------------------------------------\n",
            "Total Timesteps: 441000 Episode Num: 441 Reward: 1852.7050206444385\n",
            "Total Timesteps: 442000 Episode Num: 442 Reward: 1788.11728023636\n",
            "Total Timesteps: 443000 Episode Num: 443 Reward: 1722.624589478535\n",
            "Total Timesteps: 444000 Episode Num: 444 Reward: 1677.0640383748191\n",
            "Total Timesteps: 445000 Episode Num: 445 Reward: 1836.1619766829183\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1598.550972\n",
            "---------------------------------------\n",
            "Total Timesteps: 446000 Episode Num: 446 Reward: 1602.981011668493\n",
            "Total Timesteps: 447000 Episode Num: 447 Reward: 1777.8580347160866\n",
            "Total Timesteps: 448000 Episode Num: 448 Reward: 1760.6891615886386\n",
            "Total Timesteps: 449000 Episode Num: 449 Reward: 1750.971252252624\n",
            "Total Timesteps: 450000 Episode Num: 450 Reward: 1886.2829829598684\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1707.151770\n",
            "---------------------------------------\n",
            "Total Timesteps: 451000 Episode Num: 451 Reward: 1741.098749386061\n",
            "Total Timesteps: 452000 Episode Num: 452 Reward: 1778.8677011740635\n",
            "Total Timesteps: 453000 Episode Num: 453 Reward: 1741.4555760683295\n",
            "Total Timesteps: 454000 Episode Num: 454 Reward: 1851.3255496353586\n",
            "Total Timesteps: 455000 Episode Num: 455 Reward: 1872.2992535223902\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1939.176550\n",
            "---------------------------------------\n",
            "Total Timesteps: 456000 Episode Num: 456 Reward: 1909.52809182696\n",
            "Total Timesteps: 457000 Episode Num: 457 Reward: 1848.2997394622373\n",
            "Total Timesteps: 458000 Episode Num: 458 Reward: 1784.2960044838462\n",
            "Total Timesteps: 459000 Episode Num: 459 Reward: 1833.0909132612405\n",
            "Total Timesteps: 460000 Episode Num: 460 Reward: 1806.4826833129264\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1784.109017\n",
            "---------------------------------------\n",
            "Total Timesteps: 461000 Episode Num: 461 Reward: 1827.6664358278879\n",
            "Total Timesteps: 462000 Episode Num: 462 Reward: 1782.0446039870756\n",
            "Total Timesteps: 463000 Episode Num: 463 Reward: 1753.626515582862\n",
            "Total Timesteps: 464000 Episode Num: 464 Reward: 1816.9632434129908\n",
            "Total Timesteps: 465000 Episode Num: 465 Reward: 1852.6797001898037\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1705.982442\n",
            "---------------------------------------\n",
            "Total Timesteps: 466000 Episode Num: 466 Reward: 1667.4146180802848\n",
            "Total Timesteps: 467000 Episode Num: 467 Reward: 2016.0976456889416\n",
            "Total Timesteps: 468000 Episode Num: 468 Reward: 1797.743948059996\n",
            "Total Timesteps: 469000 Episode Num: 469 Reward: 1886.122421060339\n",
            "Total Timesteps: 470000 Episode Num: 470 Reward: 1939.3801002073956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1815.555143\n",
            "---------------------------------------\n",
            "Total Timesteps: 471000 Episode Num: 471 Reward: 1805.9466405855544\n",
            "Total Timesteps: 472000 Episode Num: 472 Reward: 1880.8680151041729\n",
            "Total Timesteps: 473000 Episode Num: 473 Reward: 1853.398314937607\n",
            "Total Timesteps: 474000 Episode Num: 474 Reward: 1820.075152954518\n",
            "Total Timesteps: 475000 Episode Num: 475 Reward: 1926.3326280394174\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1822.807155\n",
            "---------------------------------------\n",
            "Total Timesteps: 476000 Episode Num: 476 Reward: 1859.3927051533672\n",
            "Total Timesteps: 477000 Episode Num: 477 Reward: 1846.5681934487786\n",
            "Total Timesteps: 478000 Episode Num: 478 Reward: 1928.1716823205275\n",
            "Total Timesteps: 479000 Episode Num: 479 Reward: 1977.0721064273498\n",
            "Total Timesteps: 480000 Episode Num: 480 Reward: 1864.6035434992395\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1743.102719\n",
            "---------------------------------------\n",
            "Total Timesteps: 481000 Episode Num: 481 Reward: 1705.7195086900322\n",
            "Total Timesteps: 482000 Episode Num: 482 Reward: 1727.473442385332\n",
            "Total Timesteps: 483000 Episode Num: 483 Reward: 1767.6279554731843\n",
            "Total Timesteps: 484000 Episode Num: 484 Reward: 1832.3601745311994\n",
            "Total Timesteps: 485000 Episode Num: 485 Reward: 1829.8135063071823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1741.822131\n",
            "---------------------------------------\n",
            "Total Timesteps: 486000 Episode Num: 486 Reward: 1801.333909976823\n",
            "Total Timesteps: 487000 Episode Num: 487 Reward: 1884.5957594502645\n",
            "Total Timesteps: 488000 Episode Num: 488 Reward: 1864.725497824789\n",
            "Total Timesteps: 489000 Episode Num: 489 Reward: 1664.5395751926644\n",
            "Total Timesteps: 490000 Episode Num: 490 Reward: 1805.2904939026746\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1786.080639\n",
            "---------------------------------------\n",
            "Total Timesteps: 491000 Episode Num: 491 Reward: 1796.1154242059956\n",
            "Total Timesteps: 492000 Episode Num: 492 Reward: 1833.4600611302908\n",
            "Total Timesteps: 493000 Episode Num: 493 Reward: 1751.7951570801865\n",
            "Total Timesteps: 494000 Episode Num: 494 Reward: 1812.8787338827583\n",
            "Total Timesteps: 495000 Episode Num: 495 Reward: 1946.2794198205377\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1679.185477\n",
            "---------------------------------------\n",
            "Total Timesteps: 496000 Episode Num: 496 Reward: 1733.3909140741712\n",
            "Total Timesteps: 497000 Episode Num: 497 Reward: 1761.34447158974\n",
            "Total Timesteps: 498000 Episode Num: 498 Reward: 1745.223961471996\n",
            "Total Timesteps: 499000 Episode Num: 499 Reward: 1883.6088720301623\n",
            "Total Timesteps: 500000 Episode Num: 500 Reward: 1717.3147402105114\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1913.636788\n",
            "---------------------------------------\n",
            "Total Timesteps: 501000 Episode Num: 501 Reward: 1903.1419776087278\n",
            "Total Timesteps: 502000 Episode Num: 502 Reward: 1906.972646398716\n",
            "Total Timesteps: 503000 Episode Num: 503 Reward: 1922.532675842697\n",
            "Total Timesteps: 504000 Episode Num: 504 Reward: 1917.6476113941628\n",
            "Total Timesteps: 505000 Episode Num: 505 Reward: 1927.9917369341906\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1961.606988\n",
            "---------------------------------------\n",
            "Total Timesteps: 506000 Episode Num: 506 Reward: 1921.271559399216\n",
            "Total Timesteps: 507000 Episode Num: 507 Reward: 1942.5389807644199\n",
            "Total Timesteps: 508000 Episode Num: 508 Reward: 1758.5190376897535\n",
            "Total Timesteps: 509000 Episode Num: 509 Reward: 1900.5134888137416\n",
            "Total Timesteps: 510000 Episode Num: 510 Reward: 1993.7612922980466\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2011.278582\n",
            "---------------------------------------\n",
            "Total Timesteps: 511000 Episode Num: 511 Reward: 1971.680197550158\n",
            "Total Timesteps: 512000 Episode Num: 512 Reward: 1950.087755075929\n",
            "Total Timesteps: 513000 Episode Num: 513 Reward: 1945.649484969347\n",
            "Total Timesteps: 514000 Episode Num: 514 Reward: 1845.4014975000207\n",
            "Total Timesteps: 515000 Episode Num: 515 Reward: 1859.3481215862364\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1992.376392\n",
            "---------------------------------------\n",
            "Total Timesteps: 516000 Episode Num: 516 Reward: 1985.6075910566158\n",
            "Total Timesteps: 517000 Episode Num: 517 Reward: 1884.7209745417852\n",
            "Total Timesteps: 518000 Episode Num: 518 Reward: 2023.4213052455893\n",
            "Total Timesteps: 519000 Episode Num: 519 Reward: 1967.3718563329899\n",
            "Total Timesteps: 520000 Episode Num: 520 Reward: 1980.8006534437307\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2034.125523\n",
            "---------------------------------------\n",
            "Total Timesteps: 521000 Episode Num: 521 Reward: 2001.828375893215\n",
            "Total Timesteps: 522000 Episode Num: 522 Reward: 1913.5990837445304\n",
            "Total Timesteps: 523000 Episode Num: 523 Reward: 2002.4921683567104\n",
            "Total Timesteps: 524000 Episode Num: 524 Reward: 1877.0513219227205\n",
            "Total Timesteps: 525000 Episode Num: 525 Reward: 2056.756339146192\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2042.283572\n",
            "---------------------------------------\n",
            "Total Timesteps: 526000 Episode Num: 526 Reward: 2039.578299426611\n",
            "Total Timesteps: 527000 Episode Num: 527 Reward: 2030.8351014738898\n",
            "Total Timesteps: 528000 Episode Num: 528 Reward: 2044.0369991790783\n",
            "Total Timesteps: 529000 Episode Num: 529 Reward: 1984.17093035542\n",
            "Total Timesteps: 530000 Episode Num: 530 Reward: 2043.94795903704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2130.409689\n",
            "---------------------------------------\n",
            "Total Timesteps: 531000 Episode Num: 531 Reward: 2074.7023509203063\n",
            "Total Timesteps: 532000 Episode Num: 532 Reward: 2038.625747409882\n",
            "Total Timesteps: 533000 Episode Num: 533 Reward: 2057.6805729460075\n",
            "Total Timesteps: 534000 Episode Num: 534 Reward: 1999.423467828704\n",
            "Total Timesteps: 535000 Episode Num: 535 Reward: 2071.304258993008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2065.512051\n",
            "---------------------------------------\n",
            "Total Timesteps: 536000 Episode Num: 536 Reward: 2060.774521571602\n",
            "Total Timesteps: 537000 Episode Num: 537 Reward: 1975.0973041162338\n",
            "Total Timesteps: 538000 Episode Num: 538 Reward: 2004.8663709585974\n",
            "Total Timesteps: 539000 Episode Num: 539 Reward: 2074.4079152114114\n",
            "Total Timesteps: 540000 Episode Num: 540 Reward: 2035.3475765344497\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1994.621297\n",
            "---------------------------------------\n",
            "Total Timesteps: 541000 Episode Num: 541 Reward: 1993.3521281452852\n",
            "Total Timesteps: 542000 Episode Num: 542 Reward: 2046.240239212581\n",
            "Total Timesteps: 543000 Episode Num: 543 Reward: 2039.8537664318408\n",
            "Total Timesteps: 544000 Episode Num: 544 Reward: 2020.216714936565\n",
            "Total Timesteps: 545000 Episode Num: 545 Reward: 2039.9655971899247\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2062.542130\n",
            "---------------------------------------\n",
            "Total Timesteps: 546000 Episode Num: 546 Reward: 2039.5200966073726\n",
            "Total Timesteps: 547000 Episode Num: 547 Reward: 2014.2075018739768\n",
            "Total Timesteps: 548000 Episode Num: 548 Reward: 2106.176775987781\n",
            "Total Timesteps: 549000 Episode Num: 549 Reward: 2026.885351162283\n",
            "Total Timesteps: 550000 Episode Num: 550 Reward: 2036.965334528743\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2088.309556\n",
            "---------------------------------------\n",
            "Total Timesteps: 551000 Episode Num: 551 Reward: 2052.772045043524\n",
            "Total Timesteps: 552000 Episode Num: 552 Reward: 2030.6694119537378\n",
            "Total Timesteps: 553000 Episode Num: 553 Reward: 2022.2752519683374\n",
            "Total Timesteps: 554000 Episode Num: 554 Reward: 2115.385927538136\n",
            "Total Timesteps: 555000 Episode Num: 555 Reward: 2126.6500773760927\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2125.860017\n",
            "---------------------------------------\n",
            "Total Timesteps: 556000 Episode Num: 556 Reward: 2051.360158259256\n",
            "Total Timesteps: 557000 Episode Num: 557 Reward: 2136.1272909639406\n",
            "Total Timesteps: 558000 Episode Num: 558 Reward: 2094.170009196284\n",
            "Total Timesteps: 559000 Episode Num: 559 Reward: 2033.5898628714149\n",
            "Total Timesteps: 560000 Episode Num: 560 Reward: 1961.6736559292651\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1945.495307\n",
            "---------------------------------------\n",
            "Total Timesteps: 561000 Episode Num: 561 Reward: 1948.4170536561262\n",
            "Total Timesteps: 562000 Episode Num: 562 Reward: 2098.260555989802\n",
            "Total Timesteps: 563000 Episode Num: 563 Reward: 2074.16589251198\n",
            "Total Timesteps: 564000 Episode Num: 564 Reward: 2105.7030086514746\n",
            "Total Timesteps: 565000 Episode Num: 565 Reward: 2045.7277257278145\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2031.036757\n",
            "---------------------------------------\n",
            "Total Timesteps: 566000 Episode Num: 566 Reward: 2000.191059670287\n",
            "Total Timesteps: 567000 Episode Num: 567 Reward: 2023.5551460409524\n",
            "Total Timesteps: 568000 Episode Num: 568 Reward: 2136.4821567233557\n",
            "Total Timesteps: 569000 Episode Num: 569 Reward: 1741.6630988500074\n",
            "Total Timesteps: 570000 Episode Num: 570 Reward: 2090.119739083428\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2118.893091\n",
            "---------------------------------------\n",
            "Total Timesteps: 571000 Episode Num: 571 Reward: 2078.474390948472\n",
            "Total Timesteps: 572000 Episode Num: 572 Reward: 2188.154031798101\n",
            "Total Timesteps: 573000 Episode Num: 573 Reward: 2059.3482386973524\n",
            "Total Timesteps: 574000 Episode Num: 574 Reward: 2112.05651456327\n",
            "Total Timesteps: 575000 Episode Num: 575 Reward: 2140.9085249210198\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2189.671168\n",
            "---------------------------------------\n",
            "Total Timesteps: 576000 Episode Num: 576 Reward: 2123.422128078653\n",
            "Total Timesteps: 577000 Episode Num: 577 Reward: 2081.597461255979\n",
            "Total Timesteps: 578000 Episode Num: 578 Reward: 2048.1306206362683\n",
            "Total Timesteps: 579000 Episode Num: 579 Reward: 2084.033059122647\n",
            "Total Timesteps: 580000 Episode Num: 580 Reward: 2113.9287937520826\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1996.503209\n",
            "---------------------------------------\n",
            "Total Timesteps: 581000 Episode Num: 581 Reward: 2011.5254003257369\n",
            "Total Timesteps: 582000 Episode Num: 582 Reward: 2065.718721422578\n",
            "Total Timesteps: 583000 Episode Num: 583 Reward: 2159.352486093507\n",
            "Total Timesteps: 584000 Episode Num: 584 Reward: 2180.0102377498456\n",
            "Total Timesteps: 585000 Episode Num: 585 Reward: 2089.698558630063\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2191.265173\n",
            "---------------------------------------\n",
            "Total Timesteps: 586000 Episode Num: 586 Reward: 2124.43764393689\n",
            "Total Timesteps: 587000 Episode Num: 587 Reward: 2127.1567819284865\n",
            "Total Timesteps: 588000 Episode Num: 588 Reward: 2110.0846435754574\n",
            "Total Timesteps: 589000 Episode Num: 589 Reward: 2110.8190325240707\n",
            "Total Timesteps: 590000 Episode Num: 590 Reward: 2120.367098527982\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2159.267926\n",
            "---------------------------------------\n",
            "Total Timesteps: 591000 Episode Num: 591 Reward: 2090.0364785626543\n",
            "Total Timesteps: 592000 Episode Num: 592 Reward: 2166.82467625968\n",
            "Total Timesteps: 593000 Episode Num: 593 Reward: 2144.0602158090805\n",
            "Total Timesteps: 594000 Episode Num: 594 Reward: 2105.87130358477\n",
            "Total Timesteps: 595000 Episode Num: 595 Reward: 2124.050217013573\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2178.663197\n",
            "---------------------------------------\n",
            "Total Timesteps: 596000 Episode Num: 596 Reward: 2119.0889742058475\n",
            "Total Timesteps: 597000 Episode Num: 597 Reward: 2167.485698144115\n",
            "Total Timesteps: 598000 Episode Num: 598 Reward: 2080.108126881147\n",
            "Total Timesteps: 599000 Episode Num: 599 Reward: 2153.774410783083\n",
            "Total Timesteps: 600000 Episode Num: 600 Reward: 2136.4795531526743\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2232.275197\n",
            "---------------------------------------\n",
            "Total Timesteps: 601000 Episode Num: 601 Reward: 2165.343419419007\n",
            "Total Timesteps: 602000 Episode Num: 602 Reward: 2146.936998297854\n",
            "Total Timesteps: 603000 Episode Num: 603 Reward: 2163.437332866382\n",
            "Total Timesteps: 604000 Episode Num: 604 Reward: 2181.167244072121\n",
            "Total Timesteps: 605000 Episode Num: 605 Reward: 2086.482697341561\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2234.605832\n",
            "---------------------------------------\n",
            "Total Timesteps: 606000 Episode Num: 606 Reward: 2178.9969930416974\n",
            "Total Timesteps: 607000 Episode Num: 607 Reward: 2173.3835177427986\n",
            "Total Timesteps: 608000 Episode Num: 608 Reward: 2201.9579932234537\n",
            "Total Timesteps: 609000 Episode Num: 609 Reward: 2134.486145337816\n",
            "Total Timesteps: 610000 Episode Num: 610 Reward: 2135.8383296027055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2193.996278\n",
            "---------------------------------------\n",
            "Total Timesteps: 611000 Episode Num: 611 Reward: 2158.923526682908\n",
            "Total Timesteps: 612000 Episode Num: 612 Reward: 2214.8833744816466\n",
            "Total Timesteps: 613000 Episode Num: 613 Reward: 2207.1410240024466\n",
            "Total Timesteps: 614000 Episode Num: 614 Reward: 2188.8976134490244\n",
            "Total Timesteps: 615000 Episode Num: 615 Reward: 2176.0453238970204\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2224.040509\n",
            "---------------------------------------\n",
            "Total Timesteps: 616000 Episode Num: 616 Reward: 2193.9770114777925\n",
            "Total Timesteps: 617000 Episode Num: 617 Reward: 2128.5538153534308\n",
            "Total Timesteps: 618000 Episode Num: 618 Reward: 2152.678312135908\n",
            "Total Timesteps: 619000 Episode Num: 619 Reward: 2180.460220403343\n",
            "Total Timesteps: 620000 Episode Num: 620 Reward: 2137.8779956808667\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2090.838564\n",
            "---------------------------------------\n",
            "Total Timesteps: 621000 Episode Num: 621 Reward: 2056.9064861807274\n",
            "Total Timesteps: 622000 Episode Num: 622 Reward: 2238.5940354528625\n",
            "Total Timesteps: 623000 Episode Num: 623 Reward: 2206.557630021231\n",
            "Total Timesteps: 624000 Episode Num: 624 Reward: 2182.914202625332\n",
            "Total Timesteps: 625000 Episode Num: 625 Reward: 2191.5158289862256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2225.731294\n",
            "---------------------------------------\n",
            "Total Timesteps: 626000 Episode Num: 626 Reward: 2169.0025898017548\n",
            "Total Timesteps: 627000 Episode Num: 627 Reward: 2170.1470064708587\n",
            "Total Timesteps: 628000 Episode Num: 628 Reward: 2208.200155683996\n",
            "Total Timesteps: 629000 Episode Num: 629 Reward: 2178.4378976741923\n",
            "Total Timesteps: 630000 Episode Num: 630 Reward: 2092.270661508273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2274.735558\n",
            "---------------------------------------\n",
            "Total Timesteps: 631000 Episode Num: 631 Reward: 2225.310815713812\n",
            "Total Timesteps: 632000 Episode Num: 632 Reward: 2228.1977200545944\n",
            "Total Timesteps: 633000 Episode Num: 633 Reward: 2219.2552481484468\n",
            "Total Timesteps: 634000 Episode Num: 634 Reward: 2183.1270793543404\n",
            "Total Timesteps: 635000 Episode Num: 635 Reward: 2174.4284301317643\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2271.842065\n",
            "---------------------------------------\n",
            "Total Timesteps: 636000 Episode Num: 636 Reward: 2222.0425088990323\n",
            "Total Timesteps: 637000 Episode Num: 637 Reward: 2247.9782182668555\n",
            "Total Timesteps: 638000 Episode Num: 638 Reward: 2185.7533689264624\n",
            "Total Timesteps: 639000 Episode Num: 639 Reward: 2182.5037552418457\n",
            "Total Timesteps: 640000 Episode Num: 640 Reward: 2199.9754162642316\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2241.360524\n",
            "---------------------------------------\n",
            "Total Timesteps: 641000 Episode Num: 641 Reward: 2222.6726055584572\n",
            "Total Timesteps: 642000 Episode Num: 642 Reward: 2171.821215276374\n",
            "Total Timesteps: 643000 Episode Num: 643 Reward: 2167.9470014011445\n",
            "Total Timesteps: 644000 Episode Num: 644 Reward: 2189.588551712045\n",
            "Total Timesteps: 645000 Episode Num: 645 Reward: 2177.8988675134733\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2253.151205\n",
            "---------------------------------------\n",
            "Total Timesteps: 646000 Episode Num: 646 Reward: 2203.559841680203\n",
            "Total Timesteps: 647000 Episode Num: 647 Reward: 2187.219820632506\n",
            "Total Timesteps: 648000 Episode Num: 648 Reward: 2194.175230894408\n",
            "Total Timesteps: 649000 Episode Num: 649 Reward: 2198.884975641663\n",
            "Total Timesteps: 650000 Episode Num: 650 Reward: 2206.9824599314215\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2248.332119\n",
            "---------------------------------------\n",
            "Total Timesteps: 651000 Episode Num: 651 Reward: 2192.5935890758496\n",
            "Total Timesteps: 652000 Episode Num: 652 Reward: 2237.2258473407383\n",
            "Total Timesteps: 653000 Episode Num: 653 Reward: 2188.3617803987468\n",
            "Total Timesteps: 654000 Episode Num: 654 Reward: 2251.154904196608\n",
            "Total Timesteps: 655000 Episode Num: 655 Reward: 2241.274257630963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2215.450855\n",
            "---------------------------------------\n",
            "Total Timesteps: 656000 Episode Num: 656 Reward: 2169.2749788646765\n",
            "Total Timesteps: 657000 Episode Num: 657 Reward: 2231.364526730281\n",
            "Total Timesteps: 658000 Episode Num: 658 Reward: 2075.6915807482214\n",
            "Total Timesteps: 659000 Episode Num: 659 Reward: 2211.118429464202\n",
            "Total Timesteps: 660000 Episode Num: 660 Reward: 2250.677303937883\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2288.366965\n",
            "---------------------------------------\n",
            "Total Timesteps: 661000 Episode Num: 661 Reward: 2215.3663354535192\n",
            "Total Timesteps: 662000 Episode Num: 662 Reward: 2279.333980388725\n",
            "Total Timesteps: 663000 Episode Num: 663 Reward: 2201.171976290193\n",
            "Total Timesteps: 664000 Episode Num: 664 Reward: 2230.901994041517\n",
            "Total Timesteps: 665000 Episode Num: 665 Reward: 2226.080200744338\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2297.793355\n",
            "---------------------------------------\n",
            "Total Timesteps: 666000 Episode Num: 666 Reward: 2248.6910772392034\n",
            "Total Timesteps: 667000 Episode Num: 667 Reward: 2202.836930043371\n",
            "Total Timesteps: 668000 Episode Num: 668 Reward: 2238.322501639037\n",
            "Total Timesteps: 669000 Episode Num: 669 Reward: 2242.965174012349\n",
            "Total Timesteps: 670000 Episode Num: 670 Reward: 2222.4647459752364\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2284.449777\n",
            "---------------------------------------\n",
            "Total Timesteps: 671000 Episode Num: 671 Reward: 2214.1553251146147\n",
            "Total Timesteps: 672000 Episode Num: 672 Reward: 2219.7899664735255\n",
            "Total Timesteps: 673000 Episode Num: 673 Reward: 2258.7925723420863\n",
            "Total Timesteps: 674000 Episode Num: 674 Reward: 2232.9952622461524\n",
            "Total Timesteps: 675000 Episode Num: 675 Reward: 2278.3305017071607\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2309.171854\n",
            "---------------------------------------\n",
            "Total Timesteps: 676000 Episode Num: 676 Reward: 2252.9668145847427\n",
            "Total Timesteps: 677000 Episode Num: 677 Reward: 2246.1059397239487\n",
            "Total Timesteps: 678000 Episode Num: 678 Reward: 2228.2397118151657\n",
            "Total Timesteps: 679000 Episode Num: 679 Reward: 2081.705975163973\n",
            "Total Timesteps: 680000 Episode Num: 680 Reward: 2268.062556345131\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2249.540383\n",
            "---------------------------------------\n",
            "Total Timesteps: 681000 Episode Num: 681 Reward: 2191.4916170602246\n",
            "Total Timesteps: 682000 Episode Num: 682 Reward: 2247.368856278275\n",
            "Total Timesteps: 683000 Episode Num: 683 Reward: 2207.087628563923\n",
            "Total Timesteps: 684000 Episode Num: 684 Reward: 2226.7154180617404\n",
            "Total Timesteps: 685000 Episode Num: 685 Reward: 2213.6479321578754\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2318.936449\n",
            "---------------------------------------\n",
            "Total Timesteps: 686000 Episode Num: 686 Reward: 2247.8245222986275\n",
            "Total Timesteps: 687000 Episode Num: 687 Reward: 2209.469779112671\n",
            "Total Timesteps: 688000 Episode Num: 688 Reward: 2213.470851310971\n",
            "Total Timesteps: 689000 Episode Num: 689 Reward: 2277.581019367187\n",
            "Total Timesteps: 690000 Episode Num: 690 Reward: 2250.4284151583847\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2262.207541\n",
            "---------------------------------------\n",
            "Total Timesteps: 691000 Episode Num: 691 Reward: 2204.808912692858\n",
            "Total Timesteps: 692000 Episode Num: 692 Reward: 2278.226321895432\n",
            "Total Timesteps: 693000 Episode Num: 693 Reward: 2271.2502420168876\n",
            "Total Timesteps: 694000 Episode Num: 694 Reward: 2279.3537883364606\n",
            "Total Timesteps: 695000 Episode Num: 695 Reward: 2281.887264839366\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2343.084897\n",
            "---------------------------------------\n",
            "Total Timesteps: 696000 Episode Num: 696 Reward: 2298.5981202443386\n",
            "Total Timesteps: 697000 Episode Num: 697 Reward: 2249.2713832255613\n",
            "Total Timesteps: 698000 Episode Num: 698 Reward: 2197.821789330261\n",
            "Total Timesteps: 699000 Episode Num: 699 Reward: 2253.9999002922555\n",
            "Total Timesteps: 700000 Episode Num: 700 Reward: 2183.2911123076387\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2306.408690\n",
            "---------------------------------------\n",
            "Total Timesteps: 701000 Episode Num: 701 Reward: 2252.6307317234928\n",
            "Total Timesteps: 702000 Episode Num: 702 Reward: 2258.3285477078352\n",
            "Total Timesteps: 703000 Episode Num: 703 Reward: 2248.354188134421\n",
            "Total Timesteps: 704000 Episode Num: 704 Reward: 2212.2811809430027\n",
            "Total Timesteps: 705000 Episode Num: 705 Reward: 2275.590934428498\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2296.548481\n",
            "---------------------------------------\n",
            "Total Timesteps: 706000 Episode Num: 706 Reward: 2237.6367485243964\n",
            "Total Timesteps: 707000 Episode Num: 707 Reward: 2235.4549076106946\n",
            "Total Timesteps: 708000 Episode Num: 708 Reward: 2214.078953922682\n",
            "Total Timesteps: 709000 Episode Num: 709 Reward: 2259.8551320282277\n",
            "Total Timesteps: 710000 Episode Num: 710 Reward: 2263.6353818208113\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2278.019774\n",
            "---------------------------------------\n",
            "Total Timesteps: 711000 Episode Num: 711 Reward: 2202.174093834216\n",
            "Total Timesteps: 712000 Episode Num: 712 Reward: 2267.504805004103\n",
            "Total Timesteps: 713000 Episode Num: 713 Reward: 2225.28735292132\n",
            "Total Timesteps: 714000 Episode Num: 714 Reward: 2225.7426880156563\n",
            "Total Timesteps: 715000 Episode Num: 715 Reward: 2207.1843755538616\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2347.233261\n",
            "---------------------------------------\n",
            "Total Timesteps: 716000 Episode Num: 716 Reward: 2282.9830992750262\n",
            "Total Timesteps: 717000 Episode Num: 717 Reward: 2256.607725189564\n",
            "Total Timesteps: 718000 Episode Num: 718 Reward: 2255.620269741863\n",
            "Total Timesteps: 719000 Episode Num: 719 Reward: 2253.531494828517\n",
            "Total Timesteps: 720000 Episode Num: 720 Reward: 2249.0345658278216\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2328.452954\n",
            "---------------------------------------\n",
            "Total Timesteps: 721000 Episode Num: 721 Reward: 2247.98421911102\n",
            "Total Timesteps: 722000 Episode Num: 722 Reward: 2289.373302128477\n",
            "Total Timesteps: 723000 Episode Num: 723 Reward: 2276.6010083319893\n",
            "Total Timesteps: 724000 Episode Num: 724 Reward: 2241.9685819890346\n",
            "Total Timesteps: 725000 Episode Num: 725 Reward: 2230.839388380037\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2304.142653\n",
            "---------------------------------------\n",
            "Total Timesteps: 726000 Episode Num: 726 Reward: 2206.5859163966456\n",
            "Total Timesteps: 727000 Episode Num: 727 Reward: 2253.079804241909\n",
            "Total Timesteps: 728000 Episode Num: 728 Reward: 2280.433013255726\n",
            "Total Timesteps: 729000 Episode Num: 729 Reward: 2209.2431952327274\n",
            "Total Timesteps: 730000 Episode Num: 730 Reward: 2267.3496963014127\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2219.894711\n",
            "---------------------------------------\n",
            "Total Timesteps: 731000 Episode Num: 731 Reward: 2226.5855552808493\n",
            "Total Timesteps: 732000 Episode Num: 732 Reward: 2251.3308208049643\n",
            "Total Timesteps: 733000 Episode Num: 733 Reward: 2244.0297790202717\n",
            "Total Timesteps: 734000 Episode Num: 734 Reward: 2279.0447724446076\n",
            "Total Timesteps: 735000 Episode Num: 735 Reward: 2260.5322187874513\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2320.116727\n",
            "---------------------------------------\n",
            "Total Timesteps: 736000 Episode Num: 736 Reward: 2262.4399676977596\n",
            "Total Timesteps: 737000 Episode Num: 737 Reward: 2280.210263919839\n",
            "Total Timesteps: 738000 Episode Num: 738 Reward: 2282.0038278162533\n",
            "Total Timesteps: 739000 Episode Num: 739 Reward: 2289.10191844404\n",
            "Total Timesteps: 740000 Episode Num: 740 Reward: 2301.414950157346\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2315.313969\n",
            "---------------------------------------\n",
            "Total Timesteps: 741000 Episode Num: 741 Reward: 2210.9237480939296\n",
            "Total Timesteps: 742000 Episode Num: 742 Reward: 2287.689004742066\n",
            "Total Timesteps: 743000 Episode Num: 743 Reward: 2276.1554198889135\n",
            "Total Timesteps: 744000 Episode Num: 744 Reward: 2250.7373722376146\n",
            "Total Timesteps: 745000 Episode Num: 745 Reward: 2267.4244060065903\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2340.782726\n",
            "---------------------------------------\n",
            "Total Timesteps: 746000 Episode Num: 746 Reward: 2284.7325941978074\n",
            "Total Timesteps: 747000 Episode Num: 747 Reward: 2290.595168006672\n",
            "Total Timesteps: 748000 Episode Num: 748 Reward: 2238.1163977542237\n",
            "Total Timesteps: 749000 Episode Num: 749 Reward: 2276.1086780198198\n",
            "Total Timesteps: 750000 Episode Num: 750 Reward: 2218.7937317672886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2334.493030\n",
            "---------------------------------------\n",
            "Total Timesteps: 751000 Episode Num: 751 Reward: 2283.684800486294\n",
            "Total Timesteps: 752000 Episode Num: 752 Reward: 2284.8787936576173\n",
            "Total Timesteps: 753000 Episode Num: 753 Reward: 2278.170430879041\n",
            "Total Timesteps: 754000 Episode Num: 754 Reward: 2192.9245403960817\n",
            "Total Timesteps: 755000 Episode Num: 755 Reward: 2219.771421375812\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2296.730646\n",
            "---------------------------------------\n",
            "Total Timesteps: 756000 Episode Num: 756 Reward: 2242.88399039012\n",
            "Total Timesteps: 757000 Episode Num: 757 Reward: 2291.655663595421\n",
            "Total Timesteps: 758000 Episode Num: 758 Reward: 2270.5301214821384\n",
            "Total Timesteps: 759000 Episode Num: 759 Reward: 2241.370708502339\n",
            "Total Timesteps: 760000 Episode Num: 760 Reward: 2260.0733222241283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2363.071440\n",
            "---------------------------------------\n",
            "Total Timesteps: 761000 Episode Num: 761 Reward: 2316.5266353634593\n",
            "Total Timesteps: 762000 Episode Num: 762 Reward: 2275.423342491888\n",
            "Total Timesteps: 763000 Episode Num: 763 Reward: 2311.769610317408\n",
            "Total Timesteps: 764000 Episode Num: 764 Reward: 2334.6284712076317\n",
            "Total Timesteps: 765000 Episode Num: 765 Reward: 2282.470402392586\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2335.721681\n",
            "---------------------------------------\n",
            "Total Timesteps: 766000 Episode Num: 766 Reward: 2273.7715491128383\n",
            "Total Timesteps: 767000 Episode Num: 767 Reward: 2274.7196367036727\n",
            "Total Timesteps: 768000 Episode Num: 768 Reward: 2300.227751438474\n",
            "Total Timesteps: 769000 Episode Num: 769 Reward: 2300.7212577329133\n",
            "Total Timesteps: 770000 Episode Num: 770 Reward: 2240.2589485829485\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2369.966269\n",
            "---------------------------------------\n",
            "Total Timesteps: 771000 Episode Num: 771 Reward: 2283.0240652655452\n",
            "Total Timesteps: 772000 Episode Num: 772 Reward: 2285.4814351016203\n",
            "Total Timesteps: 773000 Episode Num: 773 Reward: 2263.2265439020757\n",
            "Total Timesteps: 774000 Episode Num: 774 Reward: 2300.810307098752\n",
            "Total Timesteps: 775000 Episode Num: 775 Reward: 2294.453345561014\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2351.721135\n",
            "---------------------------------------\n",
            "Total Timesteps: 776000 Episode Num: 776 Reward: 2302.8046457605265\n",
            "Total Timesteps: 777000 Episode Num: 777 Reward: 2267.042873950459\n",
            "Total Timesteps: 778000 Episode Num: 778 Reward: 2236.056596237574\n",
            "Total Timesteps: 779000 Episode Num: 779 Reward: 2271.2501917839154\n",
            "Total Timesteps: 780000 Episode Num: 780 Reward: 2316.0943757180185\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2359.324725\n",
            "---------------------------------------\n",
            "Total Timesteps: 781000 Episode Num: 781 Reward: 2279.1617261604156\n",
            "Total Timesteps: 782000 Episode Num: 782 Reward: 2218.63335925969\n",
            "Total Timesteps: 783000 Episode Num: 783 Reward: 2237.891881741353\n",
            "Total Timesteps: 784000 Episode Num: 784 Reward: 2316.5327957008576\n",
            "Total Timesteps: 785000 Episode Num: 785 Reward: 2296.206335674656\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2363.652959\n",
            "---------------------------------------\n",
            "Total Timesteps: 786000 Episode Num: 786 Reward: 2312.858047587108\n",
            "Total Timesteps: 787000 Episode Num: 787 Reward: 2263.3011877354784\n",
            "Total Timesteps: 788000 Episode Num: 788 Reward: 2287.9022583560613\n",
            "Total Timesteps: 789000 Episode Num: 789 Reward: 2310.9567364045283\n",
            "Total Timesteps: 790000 Episode Num: 790 Reward: 2296.4057408059066\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2377.804301\n",
            "---------------------------------------\n",
            "Total Timesteps: 791000 Episode Num: 791 Reward: 2309.0364324120906\n",
            "Total Timesteps: 792000 Episode Num: 792 Reward: 2308.875042372841\n",
            "Total Timesteps: 793000 Episode Num: 793 Reward: 2252.212006327528\n",
            "Total Timesteps: 794000 Episode Num: 794 Reward: 2236.0899289799395\n",
            "Total Timesteps: 795000 Episode Num: 795 Reward: 2250.5781697919506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2302.659110\n",
            "---------------------------------------\n",
            "Total Timesteps: 796000 Episode Num: 796 Reward: 2278.98977103843\n",
            "Total Timesteps: 797000 Episode Num: 797 Reward: 2290.835375170625\n",
            "Total Timesteps: 798000 Episode Num: 798 Reward: 2311.362961833534\n",
            "Total Timesteps: 799000 Episode Num: 799 Reward: 2302.127262298583\n",
            "Total Timesteps: 800000 Episode Num: 800 Reward: 2290.5596852953827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2326.342685\n",
            "---------------------------------------\n",
            "Total Timesteps: 801000 Episode Num: 801 Reward: 2259.7454335891853\n",
            "Total Timesteps: 802000 Episode Num: 802 Reward: 2263.313599095098\n",
            "Total Timesteps: 803000 Episode Num: 803 Reward: 2255.257572773331\n",
            "Total Timesteps: 804000 Episode Num: 804 Reward: 2276.5085915279005\n",
            "Total Timesteps: 805000 Episode Num: 805 Reward: 2240.06278308646\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2279.716313\n",
            "---------------------------------------\n",
            "Total Timesteps: 806000 Episode Num: 806 Reward: 2221.489717458746\n",
            "Total Timesteps: 807000 Episode Num: 807 Reward: 2288.98809875814\n",
            "Total Timesteps: 808000 Episode Num: 808 Reward: 2195.3777219824074\n",
            "Total Timesteps: 809000 Episode Num: 809 Reward: 2272.0638072299316\n",
            "Total Timesteps: 810000 Episode Num: 810 Reward: 2261.6774317922077\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2317.706919\n",
            "---------------------------------------\n",
            "Total Timesteps: 811000 Episode Num: 811 Reward: 2291.6869617955936\n",
            "Total Timesteps: 812000 Episode Num: 812 Reward: 2211.048812873777\n",
            "Total Timesteps: 813000 Episode Num: 813 Reward: 2321.786612554534\n",
            "Total Timesteps: 814000 Episode Num: 814 Reward: 2281.2692999148403\n",
            "Total Timesteps: 815000 Episode Num: 815 Reward: 2334.491383875655\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2430.238435\n",
            "---------------------------------------\n",
            "Total Timesteps: 816000 Episode Num: 816 Reward: 2383.830384236167\n",
            "Total Timesteps: 817000 Episode Num: 817 Reward: 2304.683117163716\n",
            "Total Timesteps: 818000 Episode Num: 818 Reward: 2299.7943083635223\n",
            "Total Timesteps: 819000 Episode Num: 819 Reward: 2194.8834228753976\n",
            "Total Timesteps: 820000 Episode Num: 820 Reward: 2313.6011828455516\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2366.587720\n",
            "---------------------------------------\n",
            "Total Timesteps: 821000 Episode Num: 821 Reward: 2279.7627800503205\n",
            "Total Timesteps: 822000 Episode Num: 822 Reward: 2307.6139456235132\n",
            "Total Timesteps: 823000 Episode Num: 823 Reward: 2274.905231485522\n",
            "Total Timesteps: 824000 Episode Num: 824 Reward: 2303.4559578851076\n",
            "Total Timesteps: 825000 Episode Num: 825 Reward: 2277.6688546235337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2364.766910\n",
            "---------------------------------------\n",
            "Total Timesteps: 826000 Episode Num: 826 Reward: 2290.34172946888\n",
            "Total Timesteps: 827000 Episode Num: 827 Reward: 2323.0998800885786\n",
            "Total Timesteps: 828000 Episode Num: 828 Reward: 2322.2260429505827\n",
            "Total Timesteps: 829000 Episode Num: 829 Reward: 2321.154244013841\n",
            "Total Timesteps: 830000 Episode Num: 830 Reward: 2282.2359434138766\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2357.929870\n",
            "---------------------------------------\n",
            "Total Timesteps: 831000 Episode Num: 831 Reward: 2274.5230798754246\n",
            "Total Timesteps: 832000 Episode Num: 832 Reward: 2308.4895511174777\n",
            "Total Timesteps: 833000 Episode Num: 833 Reward: 2231.3870174636013\n",
            "Total Timesteps: 834000 Episode Num: 834 Reward: 2329.9440354713647\n",
            "Total Timesteps: 835000 Episode Num: 835 Reward: 2314.0733128433535\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2368.245757\n",
            "---------------------------------------\n",
            "Total Timesteps: 836000 Episode Num: 836 Reward: 2293.414773815688\n",
            "Total Timesteps: 837000 Episode Num: 837 Reward: 2296.233341323895\n",
            "Total Timesteps: 838000 Episode Num: 838 Reward: 2302.3996442037405\n",
            "Total Timesteps: 839000 Episode Num: 839 Reward: 2320.1492655247494\n",
            "Total Timesteps: 840000 Episode Num: 840 Reward: 2254.506222737465\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2373.406302\n",
            "---------------------------------------\n",
            "Total Timesteps: 841000 Episode Num: 841 Reward: 2274.8237138558547\n",
            "Total Timesteps: 842000 Episode Num: 842 Reward: 2294.480856110918\n",
            "Total Timesteps: 843000 Episode Num: 843 Reward: 2214.617988282373\n",
            "Total Timesteps: 844000 Episode Num: 844 Reward: 2284.0048584002407\n",
            "Total Timesteps: 845000 Episode Num: 845 Reward: 2306.5635715520993\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2364.044723\n",
            "---------------------------------------\n",
            "Total Timesteps: 846000 Episode Num: 846 Reward: 2296.2496163928954\n",
            "Total Timesteps: 847000 Episode Num: 847 Reward: 2294.9460375213157\n",
            "Total Timesteps: 848000 Episode Num: 848 Reward: 2232.039902766097\n",
            "Total Timesteps: 849000 Episode Num: 849 Reward: 2324.926320256845\n",
            "Total Timesteps: 850000 Episode Num: 850 Reward: 2306.9777961552695\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2349.933356\n",
            "---------------------------------------\n",
            "Total Timesteps: 851000 Episode Num: 851 Reward: 2276.5173553474724\n",
            "Total Timesteps: 852000 Episode Num: 852 Reward: 2256.8835786047302\n",
            "Total Timesteps: 853000 Episode Num: 853 Reward: 2233.338183659833\n",
            "Total Timesteps: 854000 Episode Num: 854 Reward: 2253.3673902699597\n",
            "Total Timesteps: 855000 Episode Num: 855 Reward: 2262.634248114845\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2369.361987\n",
            "---------------------------------------\n",
            "Total Timesteps: 856000 Episode Num: 856 Reward: 2301.693876712595\n",
            "Total Timesteps: 857000 Episode Num: 857 Reward: 2289.5928096767825\n",
            "Total Timesteps: 858000 Episode Num: 858 Reward: 2215.6769795967275\n",
            "Total Timesteps: 859000 Episode Num: 859 Reward: 2249.831608034415\n",
            "Total Timesteps: 860000 Episode Num: 860 Reward: 2329.967847775318\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2355.236439\n",
            "---------------------------------------\n",
            "Total Timesteps: 861000 Episode Num: 861 Reward: 2279.7273448022024\n",
            "Total Timesteps: 862000 Episode Num: 862 Reward: 2252.2429996994456\n",
            "Total Timesteps: 863000 Episode Num: 863 Reward: 2271.3179407079756\n",
            "Total Timesteps: 864000 Episode Num: 864 Reward: 2229.580319183512\n",
            "Total Timesteps: 865000 Episode Num: 865 Reward: 2259.584091733184\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2332.828133\n",
            "---------------------------------------\n",
            "Total Timesteps: 866000 Episode Num: 866 Reward: 2270.0651649009396\n",
            "Total Timesteps: 867000 Episode Num: 867 Reward: 2272.550987178646\n",
            "Total Timesteps: 868000 Episode Num: 868 Reward: 2302.3598772451496\n",
            "Total Timesteps: 869000 Episode Num: 869 Reward: 2225.2188426457933\n",
            "Total Timesteps: 870000 Episode Num: 870 Reward: 2279.300442017881\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2360.292136\n",
            "---------------------------------------\n",
            "Total Timesteps: 871000 Episode Num: 871 Reward: 2302.0409880049906\n",
            "Total Timesteps: 872000 Episode Num: 872 Reward: 2262.710567281181\n",
            "Total Timesteps: 873000 Episode Num: 873 Reward: 2272.5738347820848\n",
            "Total Timesteps: 874000 Episode Num: 874 Reward: 2275.393509715005\n",
            "Total Timesteps: 875000 Episode Num: 875 Reward: 2311.3441171315903\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2293.127363\n",
            "---------------------------------------\n",
            "Total Timesteps: 876000 Episode Num: 876 Reward: 2209.6036117086705\n",
            "Total Timesteps: 877000 Episode Num: 877 Reward: 2303.4888619361886\n",
            "Total Timesteps: 878000 Episode Num: 878 Reward: 2292.7324434589877\n",
            "Total Timesteps: 879000 Episode Num: 879 Reward: 2292.4092285525207\n",
            "Total Timesteps: 880000 Episode Num: 880 Reward: 2241.6978764872497\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2385.182012\n",
            "---------------------------------------\n",
            "Total Timesteps: 881000 Episode Num: 881 Reward: 2311.5903790583066\n",
            "Total Timesteps: 882000 Episode Num: 882 Reward: 2269.5101287326156\n",
            "Total Timesteps: 883000 Episode Num: 883 Reward: 2187.3551974563366\n",
            "Total Timesteps: 884000 Episode Num: 884 Reward: 2305.942219349514\n",
            "Total Timesteps: 885000 Episode Num: 885 Reward: 2289.7119644665568\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2302.618204\n",
            "---------------------------------------\n",
            "Total Timesteps: 886000 Episode Num: 886 Reward: 2266.9031118979115\n",
            "Total Timesteps: 887000 Episode Num: 887 Reward: 2230.7598888522784\n",
            "Total Timesteps: 888000 Episode Num: 888 Reward: 2254.4349191007113\n",
            "Total Timesteps: 889000 Episode Num: 889 Reward: 2304.935594915805\n",
            "Total Timesteps: 890000 Episode Num: 890 Reward: 2286.26610390246\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2344.017317\n",
            "---------------------------------------\n",
            "Total Timesteps: 891000 Episode Num: 891 Reward: 2277.3729280219536\n",
            "Total Timesteps: 892000 Episode Num: 892 Reward: 2295.955202242166\n",
            "Total Timesteps: 893000 Episode Num: 893 Reward: 2271.9744242125075\n",
            "Total Timesteps: 894000 Episode Num: 894 Reward: 2311.0349213943514\n",
            "Total Timesteps: 895000 Episode Num: 895 Reward: 2323.9408920177125\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2314.048186\n",
            "---------------------------------------\n",
            "Total Timesteps: 896000 Episode Num: 896 Reward: 2271.1849544251177\n",
            "Total Timesteps: 897000 Episode Num: 897 Reward: 2314.769909815652\n",
            "Total Timesteps: 898000 Episode Num: 898 Reward: 2299.5764084308494\n",
            "Total Timesteps: 899000 Episode Num: 899 Reward: 2264.864345491025\n",
            "Total Timesteps: 900000 Episode Num: 900 Reward: 2259.7673375762743\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2261.942175\n",
            "---------------------------------------\n",
            "Total Timesteps: 901000 Episode Num: 901 Reward: 2218.430502959202\n",
            "Total Timesteps: 902000 Episode Num: 902 Reward: 2358.085173897497\n",
            "Total Timesteps: 903000 Episode Num: 903 Reward: 2280.196004763604\n",
            "Total Timesteps: 904000 Episode Num: 904 Reward: 2291.0354508345113\n",
            "Total Timesteps: 905000 Episode Num: 905 Reward: 2345.9540090259875\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2351.932791\n",
            "---------------------------------------\n",
            "Total Timesteps: 906000 Episode Num: 906 Reward: 2293.903425758352\n",
            "Total Timesteps: 907000 Episode Num: 907 Reward: 2268.2129265789085\n",
            "Total Timesteps: 908000 Episode Num: 908 Reward: 2310.974172445764\n",
            "Total Timesteps: 909000 Episode Num: 909 Reward: 2315.9473623758463\n",
            "Total Timesteps: 910000 Episode Num: 910 Reward: 2279.27686241614\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2329.544276\n",
            "---------------------------------------\n",
            "Total Timesteps: 911000 Episode Num: 911 Reward: 2270.596019862862\n",
            "Total Timesteps: 912000 Episode Num: 912 Reward: 2313.4484987145092\n",
            "Total Timesteps: 913000 Episode Num: 913 Reward: 2290.6683474035476\n",
            "Total Timesteps: 914000 Episode Num: 914 Reward: 2267.603601313665\n",
            "Total Timesteps: 915000 Episode Num: 915 Reward: 2294.1115927344963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2409.096639\n",
            "---------------------------------------\n",
            "Total Timesteps: 916000 Episode Num: 916 Reward: 2309.969710134187\n",
            "Total Timesteps: 917000 Episode Num: 917 Reward: 2268.966247394603\n",
            "Total Timesteps: 918000 Episode Num: 918 Reward: 2294.4451208684964\n",
            "Total Timesteps: 919000 Episode Num: 919 Reward: 2315.2849441668236\n",
            "Total Timesteps: 920000 Episode Num: 920 Reward: 2295.2760322624626\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2326.493605\n",
            "---------------------------------------\n",
            "Total Timesteps: 921000 Episode Num: 921 Reward: 2306.999946934233\n",
            "Total Timesteps: 922000 Episode Num: 922 Reward: 2296.32597315467\n",
            "Total Timesteps: 923000 Episode Num: 923 Reward: 2299.662502510543\n",
            "Total Timesteps: 924000 Episode Num: 924 Reward: 2307.071245321663\n",
            "Total Timesteps: 925000 Episode Num: 925 Reward: 2298.8925285524642\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2391.940510\n",
            "---------------------------------------\n",
            "Total Timesteps: 926000 Episode Num: 926 Reward: 2292.181673437964\n",
            "Total Timesteps: 927000 Episode Num: 927 Reward: 2302.6749557273783\n",
            "Total Timesteps: 928000 Episode Num: 928 Reward: 2291.506766069623\n",
            "Total Timesteps: 929000 Episode Num: 929 Reward: 2285.6309997665394\n",
            "Total Timesteps: 930000 Episode Num: 930 Reward: 2360.725765017422\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2419.361698\n",
            "---------------------------------------\n",
            "Total Timesteps: 931000 Episode Num: 931 Reward: 2322.7348740916987\n",
            "Total Timesteps: 932000 Episode Num: 932 Reward: 2276.169335758071\n",
            "Total Timesteps: 933000 Episode Num: 933 Reward: 2270.633048054943\n",
            "Total Timesteps: 934000 Episode Num: 934 Reward: 2293.014230729068\n",
            "Total Timesteps: 935000 Episode Num: 935 Reward: 2261.074073614711\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2416.311635\n",
            "---------------------------------------\n",
            "Total Timesteps: 936000 Episode Num: 936 Reward: 2316.9908487072576\n",
            "Total Timesteps: 937000 Episode Num: 937 Reward: 2298.649946158885\n",
            "Total Timesteps: 938000 Episode Num: 938 Reward: 2261.5385203122614\n",
            "Total Timesteps: 939000 Episode Num: 939 Reward: 2305.428630843316\n",
            "Total Timesteps: 940000 Episode Num: 940 Reward: 2198.103247275824\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2408.428148\n",
            "---------------------------------------\n",
            "Total Timesteps: 941000 Episode Num: 941 Reward: 2304.4387401951526\n",
            "Total Timesteps: 942000 Episode Num: 942 Reward: 2345.7077117940053\n",
            "Total Timesteps: 943000 Episode Num: 943 Reward: 2335.018708893204\n",
            "Total Timesteps: 944000 Episode Num: 944 Reward: 2295.495327367794\n",
            "Total Timesteps: 945000 Episode Num: 945 Reward: 2280.8057280189346\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2392.199087\n",
            "---------------------------------------\n",
            "Total Timesteps: 946000 Episode Num: 946 Reward: 2259.8424176994686\n",
            "Total Timesteps: 947000 Episode Num: 947 Reward: 2284.8608309594406\n",
            "Total Timesteps: 948000 Episode Num: 948 Reward: 2289.2170731515853\n",
            "Total Timesteps: 949000 Episode Num: 949 Reward: 2265.9522118630516\n",
            "Total Timesteps: 950000 Episode Num: 950 Reward: 2361.2785762018257\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2340.185934\n",
            "---------------------------------------\n",
            "Total Timesteps: 951000 Episode Num: 951 Reward: 2267.032755158417\n",
            "Total Timesteps: 952000 Episode Num: 952 Reward: 2254.304505192351\n",
            "Total Timesteps: 953000 Episode Num: 953 Reward: 2268.6618861270417\n",
            "Total Timesteps: 954000 Episode Num: 954 Reward: 2292.6167488007586\n",
            "Total Timesteps: 955000 Episode Num: 955 Reward: 2229.706546552535\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2331.966825\n",
            "---------------------------------------\n",
            "Total Timesteps: 956000 Episode Num: 956 Reward: 2284.433645701877\n",
            "Total Timesteps: 957000 Episode Num: 957 Reward: 2317.045873814097\n",
            "Total Timesteps: 958000 Episode Num: 958 Reward: 2261.136499372882\n",
            "Total Timesteps: 959000 Episode Num: 959 Reward: 2313.1624940929983\n",
            "Total Timesteps: 960000 Episode Num: 960 Reward: 2305.4827033949964\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2393.898663\n",
            "---------------------------------------\n",
            "Total Timesteps: 961000 Episode Num: 961 Reward: 2300.959555886287\n",
            "Total Timesteps: 962000 Episode Num: 962 Reward: 2289.7998988482764\n",
            "Total Timesteps: 963000 Episode Num: 963 Reward: 2261.8771525384295\n",
            "Total Timesteps: 964000 Episode Num: 964 Reward: 2307.3776134005248\n",
            "Total Timesteps: 965000 Episode Num: 965 Reward: 2303.8641211909585\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2356.373259\n",
            "---------------------------------------\n",
            "Total Timesteps: 966000 Episode Num: 966 Reward: 2297.970363611193\n",
            "Total Timesteps: 967000 Episode Num: 967 Reward: 2306.1685481387985\n",
            "Total Timesteps: 968000 Episode Num: 968 Reward: 2324.9119720644094\n",
            "Total Timesteps: 969000 Episode Num: 969 Reward: 2285.0888336856015\n",
            "Total Timesteps: 970000 Episode Num: 970 Reward: 2316.7809880670275\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2404.860881\n",
            "---------------------------------------\n",
            "Total Timesteps: 971000 Episode Num: 971 Reward: 2280.3841128177764\n",
            "Total Timesteps: 972000 Episode Num: 972 Reward: 2320.262281440297\n",
            "Total Timesteps: 973000 Episode Num: 973 Reward: 2276.686720475952\n",
            "Total Timesteps: 974000 Episode Num: 974 Reward: 2313.8952874322126\n",
            "Total Timesteps: 975000 Episode Num: 975 Reward: 2322.0213007700395\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2387.638543\n",
            "---------------------------------------\n",
            "Total Timesteps: 976000 Episode Num: 976 Reward: 2329.1469005246327\n",
            "Total Timesteps: 977000 Episode Num: 977 Reward: 2283.9046431223796\n",
            "Total Timesteps: 978000 Episode Num: 978 Reward: 2285.444943653566\n",
            "Total Timesteps: 979000 Episode Num: 979 Reward: 2309.4790281124433\n",
            "Total Timesteps: 980000 Episode Num: 980 Reward: 2308.1670119311802\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2420.878176\n",
            "---------------------------------------\n",
            "Total Timesteps: 981000 Episode Num: 981 Reward: 2334.8317093940964\n",
            "Total Timesteps: 982000 Episode Num: 982 Reward: 2286.794490424389\n",
            "Total Timesteps: 983000 Episode Num: 983 Reward: 2303.5384211649603\n",
            "Total Timesteps: 984000 Episode Num: 984 Reward: 2304.8453780815803\n",
            "Total Timesteps: 985000 Episode Num: 985 Reward: 2315.753502058079\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2332.882796\n",
            "---------------------------------------\n",
            "Total Timesteps: 986000 Episode Num: 986 Reward: 2300.8117965874444\n",
            "Total Timesteps: 987000 Episode Num: 987 Reward: 2297.8445044560663\n",
            "Total Timesteps: 988000 Episode Num: 988 Reward: 2353.041029565985\n",
            "Total Timesteps: 989000 Episode Num: 989 Reward: 2299.06663897319\n",
            "Total Timesteps: 990000 Episode Num: 990 Reward: 2304.3205919501283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2403.372113\n",
            "---------------------------------------\n",
            "Total Timesteps: 991000 Episode Num: 991 Reward: 2347.00312394537\n",
            "Total Timesteps: 992000 Episode Num: 992 Reward: 2274.501218228967\n",
            "Total Timesteps: 993000 Episode Num: 993 Reward: 2273.0258260628775\n",
            "Total Timesteps: 994000 Episode Num: 994 Reward: 2338.5385466626144\n",
            "Total Timesteps: 995000 Episode Num: 995 Reward: 2322.563319488931\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2409.441836\n",
            "---------------------------------------\n",
            "Total Timesteps: 996000 Episode Num: 996 Reward: 2338.532974958402\n",
            "Total Timesteps: 997000 Episode Num: 997 Reward: 2336.6246932290114\n",
            "Total Timesteps: 998000 Episode Num: 998 Reward: 2312.125765509127\n",
            "Total Timesteps: 999000 Episode Num: 999 Reward: 2353.450598004057\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2412.458619\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference "
      ],
      "metadata": {
        "id": "O5QtGkMjhZqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "metadata": {
        "id": "UXVCpXt6hdfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cac50f-022c-4a1d-c223-883382f0a395"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2425.974545\n",
            "---------------------------------------\n"
          ]
        }
      ]
    }
  ]
}